@InProceedings{Kamphuis2020BM25,
	author="Kamphuis, Chris
	and de Vries, Arjen P.
	and Boytsov, Leonid
	and Lin, Jimmy",
	title={{Which BM25 Do You Mean? A Large-Scale Reproducibility Study of Scoring Variants}},
	booktitle="Advances in Information Retrieval",
	year="2020",
	publisher="Springer International Publishing",
	address="Cham",
	pages="28--34",
	abstract="When researchers speak of BM25, it is not entirely clear which variant they mean, since many tweaks to Robertson et al.'s original formulation have been proposed. When practitioners speak of BM25, they most likely refer to the implementation in the Lucene open-source search library. Does this ambiguity ``matter''? We attempt to answer this question with a large-scale reproducibility study of BM25, considering eight variants. Experiments on three newswire collections show that there are no significant effectiveness differences between them, including Lucene's often maligned approximation of document length. As an added benefit, our empirical approach takes advantage of databases for rapid IR prototyping, which validates both the feasibility and methodological advantages claimed in previous work.",
	isbn="978-3-030-45442-5",
	series="ECIR '20"
}

@inproceedings{entity-cards,
	author = {Hasibi, Faegheh and Balog, Krisztian and Bratsberg, Svein Erik},
	title = {Dynamic Factual Summaries for Entity Cards},
	year = {2017},
	isbn = {9781450350228},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3077136.3080810},
	doi = {10.1145/3077136.3080810},
	abstract = {Entity cards are being used frequently in modern web search engines to offer a concise overview of an entity directly on the results page. These cards are composed of various elements, one of them being the entity summary: a selection of facts describing the entity from an underlying knowledge base. These summaries, while presenting a synopsis of the entity, can also directly address users' information needs. In this paper, we make the first effort towards generating and evaluating such factual summaries. We introduce and address the novel problem of dynamic entity summarization for entity cards, and break it down to two specific subtasks: fact ranking and summary generation. We perform an extensive evaluation of our method using crowdsourcing. Our results show the effectiveness of our fact ranking approach and validate that users prefer dynamic summaries over static ones.},
	booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	pages = {773–782},
	numpages = {10},
	keywords = {entity cards, user interfaces, entity summarization},
	location = {Shinjuku, Tokyo, Japan},
	series = {SIGIR '17}
}

@inproceedings{OldDog,
	author = {M\"{u}hleisen, Hannes and Samar, Thaer and Lin, Jimmy and de Vries, Arjen P.},
	title = {{Old Dogs Are Great at New Tricks: Column Stores for IR Prototyping}},
	year = {2014},
	isbn = {9781450322577},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	doi = {10.1145/2600428.2609460},
	abstract = {We make the suggestion that instead of implementing custom index structures and query evaluation algorithms, IR researchers should simply store document representations in a column-oriented relational database and implement ranking models using SQL. For rapid prototyping, this is particularly advantageous since researchers can explore new scoring functions and features by simply issuing SQL queries, without needing to write imperative code. We demonstrate the feasibility of this approach by an implementation of conjunctive BM25 using two modern column stores. Experiments on a web collection show that a retrieval engine built in this manner achieves effectiveness and efficiency on par with custom-built retrieval engines, but provides many additional advantages, including cleaner query semantics, a simpler architecture, built-in support for error analysis, and the ability to exploit advances in database technology "for free".},
	booktitle = {Proceedings of the 37th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	pages = {863–866},
	numpages = {4},
	keywords = {relational databases, bm25},
	location = {Gold Coast, Queensland, Australia},
	series = {SIGIR '14}
}

@inproceedings{OSIRRC,
	author = {Clancy, Ryan and Ferro, Nicola and Hauff, Claudia and Lin, Jimmy and Sakai, Tetsuya and Wu, Ze Zhong},
	title = {{The SIGIR 2019 Open-Source IR Replicability Challenge (OSIRRC 2019)}},
	year = {2019},
	isbn = {9781450361729},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	doi = {10.1145/3331184.3331647},
	abstract = {The importance of repeatability, replicability, and reproducibility is broadly recognized in the computational sciences, both in supporting desirable scientific methodology as well as sustaining empirical progress. This workshop tackles the replicability challenge for ad hoc document retrieval, via a common Docker interface specification to support images that capture systems performing ad hoc retrieval experiments on standard test collections.},
	booktitle = {Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval},
	pages = {1432–1434},
	numpages = {3},
	keywords = {docker, ad hoc retrieval, test collections},
	location = {Paris, France},
	series = {SIGIR'19}
}

@article{RIGOR,
	author = {Arguello, Jaime and Crane, Matt and Diaz, Fernando and Lin, Jimmy and Trotman, Andrew},
	title = {{Report on the SIGIR 2015 Workshop on Reproducibility, Inexplicability, and Generalizability of Results (RIGOR)}},
	year = {2016},
	issue_date = {December 2015},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {49},
	number = {2},
	issn = {0163-5840},
	doi = {10.1145/2888422.2888439},
	abstract = {The SIGIR 2015 Workshop on Reproducibility, Inexplicability, and Generalizability of Results (RIGOR) took place on Thursday, August 13, 2015 in Santiago, Chile. The goal of the workshop was two fold. The first to provide a venue for the publication and presentation of negative results. The second was to provide a venue through which the authors of open source search engines could compare performance of indexing and searching on the same collections and on the same machines - encouraging the sharing of ideas and discoveries in a like-to-like environment. In total three papers were presented and seven systems participated.},
	journal = {SIGIR Forum},
	month = {jan},
	pages = {107–116},
	numpages = {10}
}
@inproceedings{ATIRE,
	title={{Towards an Efficient and Effective Search Engine.}},
	author={Trotman, Andrew and Jia, Xiangfei and Crane, Matt},
	series = {OSIR@ SIGIR'12},
	booktitle={Proceedings of the SIGIR 2012 Workshop on Open Source Information Retrieval},
	pages={40--47},
	location = {Portland, Oregon, USA},
	year={2012}
}
@inproceedings{trotman-bm25,
	author = {Trotman, Andrew and Puurula, Antti and Burgess, Blake},
	title = {{Improvements to BM25 and Language Models Examined}},
	year = {2014},
	isbn = {9781450330008},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	doi = {10.1145/2682862.2682863},
	abstract = {Recent work on search engine ranking functions report improvements on BM25 and Language Models with Dirichlet Smoothing. In this investigation 9 recent ranking functions (BM25, BM25+, BM25T, BM25-adpt, BM25L, TF1°δ°p\texttimes{}ID, LM-DS, LM-PYP, and LM-PYP-TFIDF) are compared by training on the INEX 2009 Wikipedia collection and testing on INEX 2010 and 9 TREC collections. We find that once trained (using particle swarm optimization) there is very little difference in performance between these functions, that relevance feedback is effective, that stemming is effective, and that it remains unclear which function is best over-all.},
	booktitle = {Proceedings of the 2014 Australasian Document Computing Symposium},
	pages = {58–65},
	numpages = {8},
	keywords = {Relevance Ranking, Document Retrieval, Procrastination},
	location = {Melbourne, VIC, Australia},
	series = {ADCS '14}
}
@inproceedings{bm25-robertson,
	title={{Okapi at TREC-3}},
	author={Stephen E. Robertson and Steve Walker and Susan Jones and Micheline Hancock-Beaulieu and Mike Gatford},
	booktitle={Overview of the third text Retrieval conference},
	year={1994},
	publisher={[Sl]: NIST},
	series={TREC-3},
	url={https://trec.nist.gov/pubs/trec3/papers/city.ps.gz},
	address={Gaithersburg, Maryland, USA}
}

@inproceedings{bm25l,
	author = {Lv, Yuanhua and Zhai, ChengXiang},
	title = {{When Documents Are Very Long, BM25 Fails!}},
	year = {2011},
	isbn = {9781450307574},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	doi = {10.1145/2009916.2010070},
	abstract = {We reveal that the Okapi BM25 retrieval function tends to overly penalize very long documents. To address this problem, we present a simple yet effective extension of BM25, namely BM25L, which "shifts" the term frequency normalization formula to boost scores of very long documents. Our experiments show that BM25L, with the same computation cost, is more effective and robust than the standard BM25.},
	booktitle = {Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	pages = {1103–1104},
	numpages = {2},
	keywords = {term frequency, very long documents, bm25, bm25l},
	location = {Beijing, China},
	series = {SIGIR '11}
}
@inproceedings{bm25+,
	author = {Lv, Yuanhua and Zhai, ChengXiang},
	title = {{Lower-Bounding Term Frequency Normalization}},
	year = {2011},
	isbn = {9781450307178},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	doi = {10.1145/2063576.2063584},
	abstract = {In this paper, we reveal a common deficiency of the current retrieval models: the component of term frequency (TF) normalization by document length is not lower-bounded properly; as a result, very long documents tend to be overly penalized. In order to analytically diagnose this problem, we propose two desirable formal constraints to capture the heuristic of lower-bounding TF, and use constraint analysis to examine several representative retrieval functions. Analysis results show that all these retrieval functions can only satisfy the constraints for a certain range of parameter values and/or for a particular set of query terms. Empirical results further show that the retrieval performance tends to be poor when the parameter is out of the range or the query term is not in the particular set. To solve this common problem, we propose a general and efficient method to introduce a sufficiently large lower bound for TF normalization which can be shown analytically to fix or alleviate the problem. Our experimental results demonstrate that the proposed method, incurring almost no additional computational cost, can be applied to state-of-the-art retrieval functions, such as Okapi BM25, language models, and the divergence from randomness approach, to significantly improve the average precision, especially for verbose queries.},
	booktitle = {Proceedings of the 20th ACM International Conference on Information and Knowledge Management},
	pages = {7–16},
	numpages = {10},
	keywords = {formal constraints, BM25+, Pl2+, term frequency, document length, data analysis, lower bound, DIR+},
	location = {Glasgow, Scotland, UK},
	series = {CIKM '11}
}
@inproceedings{bm25-adpt,
	author = {Lv, Yuanhua and Zhai, ChengXiang},
	title = {{Adaptive Term Frequency Normalization for BM25}},
	year = {2011},
	isbn = {9781450307178},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	doi = {10.1145/2063576.2063871},
	abstract = {A key component of BM25 contributing to its success is its sub linear term frequency (TF) normalization formula. The scale and shape of this TF normalization component is controlled by a parameter k1, which is generally set to a term-independent constant. We hypothesize and show empirically that in order to optimize retrieval performance, this parameter should be set in a term-specific way. Following this intuition, we propose an information gain measure to directly estimate the contributions of repeated term occurrences, which is then exploited to fit the BM25 function to predict a term-specific k1. Our experiment results show that the proposed approach, without needing any training data, can efficiently and automatically estimate a term-specific k1, and is more effective and robust than the standard BM25.},
	booktitle = {Proceedings of the 20th ACM International Conference on Information and Knowledge Management},
	pages = {1985–1988},
	numpages = {4},
	keywords = {bm25, information gain, term frequency, adaptation},
	location = {Glasgow, Scotland, UK},
	series = {CIKM '11}
}
@inproceedings{tf-ldp-idf,
	author = {Rousseau, Fran\c{c}ois and Vazirgiannis, Michalis},
	title = {{Composition of TF Normalizations: New Insights on Scoring Functions for Ad Hoc IR}},
	year = {2013},
	isbn = {9781450320344},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	doi = {10.1145/2484028.2484121},
	abstract = {Previous papers in ad hoc IR reported that scoring functions should satisfy a set of heuristic retrieval constraints, providing a mathematical justification for the normalizations historically applied to the term frequency (TF). In this paper, we propose a further level of abstraction, claiming that the successive normalizations are carried out through composition. Thus we introduce a principled framework that fully explains BM25 as a variant of TF-IDF with an inverse order of function composition. Our experiments over standard datasets indicate that the respective orders of composition chosen in the original papers for both TF-IDF and BM25 are the most effective ones. Moreover, since the order is different between the two models, they also demonstrated that the order is instrumental in the design of weighting models. In fact, while considering more complex scoring functions such as BM25+, we discovered a novel weighting model in terms of order of composition that consistently outperforms all the rest. Our contribution here is twofold: we provide a unifying mathematical framework for IR and a novel scoring function discovered using this framework.},
	booktitle = {Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	pages = {917–920},
	numpages = {4},
	keywords = {scoring functions, tf normalizations, function composition, ir theory, heuristic retrieval constraints},
	location = {Dublin, Ireland},
	series = {SIGIR '13}
}
@inproceedings{SchekPistor,
	author = {Schek, Hans-J\"{o}rg and Pistor, Peter},
	title = {{Data Structures for an Integrated Data Base Management and Information Retrieval System}},
	year = {1982},
	isbn = {0934613141},
	publisher = {Morgan Kaufmann Publishers Inc.},
	address = {San Francisco, CA, USA},
	booktitle = {Proceedings of the 8th International Conference on Very Large Data Bases},
	pages = {197–207},
	numpages = {11},
	series = {VLDB '82}
}
@article{macleod,
	author = {Macleod, Ian A.},
	title = {{Text Retrieval and the Relational Model}},
	journal = {Journal of the American Society for Information Science},
	volume = {42},
	number = {3},
	pages = {155-165},
	doi = {https://doi.org/10.1002/(SICI)1097-4571(199104)42:3<155::AID-ASI1>3.0.CO;2-H},
	eprint = {https://asistdl.onlinelibrary.wiley.com/doi/pdf/10.1002/%28SICI%291097-4571%28199104%2942%3A3%3C155%3A%3AAID-ASI1%3E3.0.CO%3B2-H},
	abstract = {Abstract In this article, the suitability of the relational model as a basis for storing and retrieving documents is examined. The relational model is compared with the conventional retrieval systems and the strengths and weaknesses of both are compared. Finally some emerging trends in document processing are discussed and some proposals are made regarding the future directions of document management systems. © 1991 John Wiley \& Sons, Inc.},
	year = {1991}
}
@article{fuhr1996probabilistic,
	title={{Models for Integrated Information Retrieval and Database Systems}},
	author={Fuhr, Norbert},
	journal={IEEE Data Engineering Bulletin},
	volume={19},
	number={1},
	pages={3--13},
	year={1996}
}
@InProceedings{dense-retrieval-1,
	author="Gao, Luyu
	and Dai, Zhuyun
	and Chen, Tongfei
	and Fan, Zhen
	and Van Durme, Benjamin
	and Callan, Jamie",
	title={{Complement Lexical Retrieval Model with Semantic Residual Embeddings}},
	booktitle="Advances in  Information Retrieval",
	year="2021",
	publisher="Springer International Publishing",
	address="Cham",
	pages="146--160",
	abstract="This paper presents clear, a retrieval model that seeks to complement classical lexical exact-match models such as BM25 with semantic matching signals from a neural embedding matching model.clear explicitly trains the neural embedding to encode language structures and semantics that lexical retrieval fails to capture with a novel residual-based embedding learning method. Empirical evaluations demonstrate the advantages of clear over state-of-the-art retrieval models, and that it can substantially improve the end-to-end accuracy and efficiency of reranking pipelines.",
	isbn="978-3-030-72113-8",
	series={ECIR '21}
}

@article{dense-retrieval-2,
	title={{Sparse, Dense, and Attentional Representations for Text Retrieval}},
	author={Luan, Yi and Eisenstein, Jacob and Toutanova, Kristina and Collins, Michael},
	journal={Transactions of the Association for Computational Linguistics},
	volume={9},
	pages={329--345},
	year={2021},
	publisher={MIT Press}
}

@article{dense-retrieval-3,
	author    = {Sheng{-}Chieh Lin and Jheng{-}Hong Yang and Jimmy Lin},
	title     = {{Distilling Dense Representations for Ranking using Tightly-Coupled Teachers}},
	journal   = {CoRR},
	volume    = {abs/2010.11386},
	year      = {2020},
	url       = {https://arxiv.org/abs/2010.11386},
	archivePrefix = {arXiv},
	eprint    = {2010.11386},
	timestamp = {Mon, 26 Oct 2020 15:39:44 +0100},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	numpages={7}
}

@inproceedings{entity-1,
	author = {Hasibi, Faegheh and Balog, Krisztian and Bratsberg, Svein Erik},
	title = {{Exploiting Entity Linking in Queries for Entity Retrieval}},
	year = {2016},
	isbn = {9781450344975},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	doi = {10.1145/2970398.2970406},
	abstract = {The premise of entity retrieval is to better answer search queries by returning specific entities instead of documents. Many queries mention particular entities; recognizing and linking them to the corresponding entry in a knowledge base is known as the task of entity linking in queries. In this paper we make a first attempt at bringing together these two, i.e., leveraging entity annotations of queries in the entity retrieval model. We introduce a new probabilistic component and show how it can be applied on top of any term-based entity retrieval model that can be emulated in the Markov Random Field framework, including language models, sequential dependence models, as well as their fielded variations. Using a standard entity retrieval test collection, we show that our extension brings consistent improvements over all baseline methods, including the current state-of-the-art. We further show that our extension is robust against parameter settings.},
	booktitle = {Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval},
	pages = {209–218},
	numpages = {10},
	keywords = {semistructured retrieval, entity linking, entity retrieval},
	location = {Newark, Delaware, USA},
	series = {ICTIR '16}
}

@book{entity-2,
	title={{Entity-oriented search}},
	author={Balog, Krisztian},
	year={2018},
	publisher={Springer Nature},
	address={Gewerbestrasse 11, 6330 Cham, Switzerland}
}

@inproceedings{entity-3,
	author = {Dalton, Jeffrey and Dietz, Laura and Allan, James},
	title = {{Entity Query Feature Expansion Using Knowledge Base Links}},
	year = {2014},
	isbn = {9781450322577},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	doi = {10.1145/2600428.2609628},
	abstract = {Recent advances in automatic entity linking and knowledge base construction have resulted in entity annotations for document and query collections. For example, annotations of entities from large general purpose knowledge bases, such as Freebase and the Google Knowledge Graph. Understanding how to leverage these entity annotations of text to improve ad hoc document retrieval is an open research area. Query expansion is a commonly used technique to improve retrieval effectiveness. Most previous query expansion approaches focus on text, mainly using unigram concepts. In this paper, we propose a new technique, called entity query feature expansion (EQFE) which enriches the query with features from entities and their links to knowledge bases, including structured attributes and text. We experiment using both explicit query entity annotations and latent entities. We evaluate our technique on TREC text collections automatically annotated with knowledge base entity links, including the Google Freebase Annotations (FACC1) data. We find that entity-based feature expansion results in significant improvements in retrieval effectiveness over state-of-the-art text expansion approaches.},
	booktitle = {Proceedings of the 37th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	pages = {365–374},
	numpages = {10},
	keywords = {entities, ontologies, information extraction, information retrieval},
	location = {Gold Coast, Queensland, Australia},
	series = {SIGIR '14}
}

@inproceedings{ltr-1,
	author = {Deveaud, Romain and Albakour, M-Dyaa and Macdonald, Craig and Ounis, Iadh},
	title = {{On the Importance of Venue-Dependent Features for Learning to Rank Contextual Suggestions}},
	year = {2014},
	isbn = {9781450325981},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	doi = {10.1145/2661829.2661956},
	abstract = {Suggesting venues to a user in a given geographic context is an emerging task that is currently attracting a lot of attention. Existing studies in the literature consist of approaches that rank candidate venues based on different features of the venues and the user, which either focus on modeling the preferences of the user or the quality of the venue. However, while providing insightful results and conclusions, none of these studies have explored the relative effectiveness of these different features. In this paper, we explore a variety of user-dependent and venue-dependent features and apply state-of-the-art learning to rank approaches to the problem of contextual suggestion in order to find what makes a venue relevant for a given context. Using the test collection of the TREC 2013 Contextual Suggestion track, we perform a number of experiments to evaluate our approach. Our results suggest that a learning to rank technique can significantly outperform a Language Modelling baseline that models the positive and negative preferences of the user. Moreover, despite the fact that the contextual suggestion task is a personalisation task (i.e. providing the user with personalised suggestions of venues), we surprisingly find that user-dependent features are less effective than venue-dependent features for estimating the relevance of a suggestion.},
	booktitle = {Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management},
	pages = {1827–1830},
	numpages = {4},
	keywords = {venue recommendation, contextual suggestion, personalisation, learning to rank},
	location = {Shanghai, China},
	series = {CIKM '14}
}

@inproceedings{ltr-2,
	author = {Macdonald, Craig and Santos, Rodrygo L.T. and Ounis, Iadh},
	title = {{On the Usefulness of Query Features for Learning to Rank}},
	year = {2012},
	isbn = {9781450311564},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	doi = {10.1145/2396761.2398691},
	abstract = {Learning to rank studies have mostly focused on query-dependent and query-independent document features, which enable the learning of ranking models of increased effectiveness. Modern learning to rank techniques based on regression trees can support query features, which are document-independent, and hence have the same values for all documents being ranked for a query. In doing so, such techniques are able to learn sub-trees that are specific to certain types of query. However, it is unclear which classes of features are useful for learning to rank, as previous studies leveraged anonymised features. In this work, we examine the usefulness of four classes of query features, based on topic classification, the history of the query in a query log, the predicted performance of the query, and the presence of concepts such as persons and organisations in the query. Through experiments on the ClueWeb09 collection, our results using a state-of-the-art learning to rank technique based on regression trees show that all four classes of query features can significantly improve upon an effective learned model that does not use any query feature.},
	booktitle = {Proceedings of the 21st ACM International Conference on Information and Knowledge Management},
	pages = {2559–2562},
	numpages = {4},
	keywords = {query features, learning to rank},
	location = {Maui, Hawaii, USA},
	series = {CIKM '12}
}

@inproceedings{angles2018property,
	title={{The Property Graph Database Model.}},
	author={Angles, Renzo},
	booktitle={Proceedings of the 12th Alberto Mendelzon International Workshop on Foundations of Data Management},
	year={2018},
	series={AMW '18},
	location={Cali, Colombia},
	publisher={CEUR-WS.org},
	address={Aachen},
	numpages={10}
}

@inproceedings{duckdb,
	author = {Raasveldt, Mark and M\"{u}hleisen, Hannes},
	title = {{DuckDB: An Embeddable Analytical Database}},
	year = {2019},
	isbn = {9781450356435},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	doi = {10.1145/3299869.3320212},
	abstract = {The immense popularity of SQLite shows that there is a need for unobtrusive in-process data management solutions. However, there is no such system yet geared towards analytical workloads. We demonstrate DuckDB, a novel data management system designed to execute analytical SQL queries while embedded in another process. In our demonstration, we pit DuckDB against other data management solutions to showcase its performance in the embedded analytics scenario. DuckDB is available as Open Source software under a permissive license.},
	booktitle = {Proceedings of the 2019 International Conference on Management of Data},
	pages = {1981–1984},
	numpages = {4},
	location = {Amsterdam, Netherlands},
	series = {SIGMOD '19}
}

@inproceedings{olddog-docker,
	author    = {Chris Kamphuis and Arjen P. de Vries},
	title     = {{The OldDog Docker Image for OSIRRC at SIGIR 2019}},
	booktitle = {Proceedings of the Open-Source {IR} Replicability Challenge co-located
	with 42nd International {ACM} {SIGIR} Conference on Research and Development
	in Information Retrieval, {OSIRRC}@{SIGIR} 2019, Paris, France, July 25,
	2019},
	pages     = {47--49},
	year      = {2019},
	url       = {http://ceur-ws.org/Vol-2409/docker07.pdf},
	timestamp = {Fri, 30 Aug 2019 13:15:06 +0200},
	address={Aachen},
	publisher={CEUR-WS.org}
}

@inproceedings{ielab,
	author    = {Harrisen Scells and Guido Zuccon},
	title     = {{ielab at the Open-Source IR Replicability Challenge 2019}},
	booktitle = {Proceedings of the Open-Source {IR} Replicability Challenge co-located
	with 42nd International {ACM} {SIGIR} Conference on Research and Development
	in Information Retrieval, {OSIRRC}@{SIGIR} 2019, Paris, France, July 25,
	2019},
	pages     = {57--61},
	year      = {2019},
	url       = {http://ceur-ws.org/Vol-2409/docker09.pdf},
	timestamp = {Fri, 30 Aug 2019 13:15:06 +0200},
	address={Aachen},
	publisher={CEUR-WS.org}
}


@inproceedings{indri-docker,
	author    = {Claudia Hauff},
	title     = {{Dockerizing Indri for OSIRRC 2019}},
	booktitle = {Proceedings of the Open-Source {IR} Replicability Challenge co-located
	with 42nd International {ACM} {SIGIR} Conference on Research and Development
	in Information Retrieval, {OSIRRC}@{SIGIR} 2019, Paris, France, July 25,
	2019},
	pages     = {44--46},
	year      = {2019},
	url       = {https://ceur-ws.org/Vol-2409/docker06.pdf},
	timestamp = {Fri, 30 Aug 2019 13:15:06 +0200},
	address={Aachen},
	publisher={CEUR-WS.org}
}



@inproceedings{anserini-docker,
	author    = {Ryan Clancy and Zeynep {Akkalyoncu Yilmaz} and Ze Zhong Wu and Jimmy Lin},
	title     = {{University of Waterloo Docker Images for OSIRRC at SIGIR 2019}},
	booktitle = {Proceedings of the Open-Source {IR} Replicability Challenge co-located
	with 42nd International {ACM} {SIGIR} Conference on Research and Development
	in Information Retrieval, {OSIRRC}@{SIGIR} 2019, Paris, France, July 25,
	2019},
	pages     = {36},
	year      = {2019},
	url       = {https://ceur-ws.org/Vol-2409/docker04.pdf},
	timestamp = {Fri, 30 Aug 2019 13:15:06 +0200},
	address={Aachen},
	publisher={CEUR-WS.org}
}

@inproceedings{terrier-docker,
	author    = {Arthur C\^{a}mara and Craig Macdonald},
	title     = {{Dockerising Terrier for The Open-Source IR Replicability Challenge (OSIRRC 2019)}},
	booktitle = {Proceedings of the Open-Source {IR} Replicability Challenge co-located
	with 42nd International {ACM} {SIGIR} Conference on Research and Development
	in Information Retrieval, {OSIRRC}@{SIGIR} 2019, Paris, France, July 25,
	2019},
	pages     = {26--30},
	year      = {2019},
	url       = {https://ceur-ws.org/Vol-2409/docker02.pdf},
	timestamp = {Fri, 30 Aug 2019 13:15:06 +0200},
	address={Aachen},
	publisher={CEUR-WS.org}
}

@inproceedings{pisa,
	author    = {Antonio Mallia and Michał Siedlaczek and Joel Mackenzie and Torsten Suel},
	title     = {{PISA: Performant Indexes and Search for Academia}},
	booktitle = {Proceedings of the Open-Source {IR} Replicability Challenge co-located
	with 42nd International {ACM} {SIGIR} Conference on Research and Development
	in Information Retrieval, {OSIRRC}@{SIGIR} 2019, Paris, France, July 25,
	2019},
	pages     = {50--56},
	year      = {2019},
	url       = {https://ceur-ws.org/Vol-2409/docker08.pdf},
	timestamp = {Fri, 30 Aug 2019 13:15:06 +0200},
	address={Aachen},
	publisher={CEUR-WS.org}
}

@inproceedings{terrier,
	author="Ounis, Iadh
	and Amati, Gianni
	and Plachouras, Vassilis
	and He, Ben
	and Macdonald, Craig
	and Johnson, Douglas",
	title={{Terrier Information Retrieval Platform}},
	booktitle="Advances in Information Retrieval",
	year="2005",
	publisher="Springer Berlin Heidelberg",
	address="Berlin, Heidelberg",
	pages="517--519",
	abstract="Terrier is a modular platform for the rapid development of large-scale Information Retrieval (IR) applications. It can index various document collections, including TREC and Web collections. Terrier also offers a range of document weighting and query expansion models, based on the Divergence From Randomness framework. It has been successfully used for ad-hoc retrieval, cross-language retrieval, Web IR and intranet search, in a centralised or distributed setting.",
	isbn="978-3-540-31865-1"
}

@inproceedings{pyserini,
	author = {Lin, Jimmy and Ma, Xueguang and Lin, Sheng-Chieh and Yang, Jheng-Hong and Pradeep, Ronak and Nogueira, Rodrigo},
	title = {{Pyserini: A Python Toolkit for Reproducible Information Retrieval Research with Sparse and Dense Representations}},
	year = {2021},
	isbn = {9781450380379},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	doi = {10.1145/3404835.3463238},
	abstract = {Pyserini is a Python toolkit for reproducible information retrieval research with sparse and dense representations. It aims to provide effective, reproducible, and easy-to-use first-stage retrieval in a multi-stage ranking architecture. Our toolkit is self-contained as a standard Python package and comes with queries, relevance judgments, pre-built indexes, and evaluation scripts for many commonly used IR test collections. We aim to support, out of the box, the entire research lifecycle of efforts aimed at improving ranking with modern neural approaches. In particular, Pyserini supports sparse retrieval (e.g., BM25 scoring using bag-of-words representations), dense retrieval (e.g., nearest-neighbor search on transformer-encoded representations), as well as hybrid retrieval that integrates both approaches. This paper provides an overview of toolkit features and presents empirical results that illustrate its effectiveness on two popular ranking tasks. Around this toolkit, our group has built a culture of reproducibility through shared norms and tools that enable rigorous automated testing.},
	booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	pages = {2356–2362},
	numpages = {7},
	keywords = {first-stage retrieval, open-source search engine},
	location = {Virtual Event, Canada},
	series = {SIGIR '21}
}

@inproceedings{Gerritse:2020:GEER,
	author = {Gerritse, Emma J. and Hasibi, Faegheh and de Vries, Arjen P.},
	title = {{Graph-Embedding Empowered Entity Retrieval}},
	year = {2020},
	isbn = {978-3-030-45438-8},
	publisher = {Springer-Verlag},
	address = {Berlin, Heidelberg},
	doi = {10.1007/978-3-030-45439-5_7},
	abstract = {In this research, we improve upon the current state of the art in entity retrieval by re-ranking the result list using graph embeddings. The paper shows that graph embeddings are useful for entity-oriented search tasks. We demonstrate empirically that encoding information from the knowledge graph into (graph) embeddings contributes to a higher increase in effectiveness of entity retrieval results than using plain word embeddings. We analyze the impact of the accuracy of the entity linker on the overall retrieval effectiveness. Our analysis further deploys the cluster hypothesis to explain the observed advantages of graph embeddings over the more widely used word embeddings, for user tasks involving ranking entities.},
	booktitle = {Advances in Information Retrieval: 42nd European Conference on IR Research, ECIR 2020, Lisbon, Portugal, April 14–17, 2020, Proceedings, Part I},
	pages = {97–110},
	numpages = {14},
	keywords = {Word embeddings, Entity retrieval, Graph embeddings},
	location = {Lisbon, Portugal}
}

@inproceedings{msmarco,
	title={{MS MARCO}: {A} {H}uman {G}enerated {MA}chine {R}eading {CO}mprehension {D}ataset},
	author={Payal Bajaj and Daniel Campos and Nick Craswell and Li Deng and Jianfeng Gao and Xiaodong Liu and Rangan Majumder and Andrew McNamara, Bhaskar Mitra and Tri Nguyen and Mir Rosenberg and Xia Song and Alina Stoica and Saurabh Tiwary and Tong Wang},
	booktitle={InCoCo@NIPS},
	pages={},
	year={2016}
}

@inproceedings{Gerritse:2022:EMBERT,
	author = {Gerritse, Emma J. and Hasibi, Faegheh and de Vries, Arjen P.},
	title = {{Entity-Aware Transformers for Entity Search}},
	year = {2022},
	isbn = {9781450387323},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	doi = {10.1145/3477495.3531971},
	abstract = {Pre-trained language models such as BERT have been a key ingredient to achieve state-of-the-art results on a variety of tasks in natural language processing and, more recently, also in information retrieval. Recent research even claims that BERT is able to capture factual knowledge about entity relations and properties, the information that is commonly obtained from knowledge graphs. This paper investigates the following question: Do BERT-based entity retrieval models benefit from additional entity information stored in knowledge graphs? To address this research question, we map entity embeddings into the same input space as a pre-trained BERT model and inject these entity embeddings into the BERT model. This entity-enriched language model is then employed on the entity retrieval task. We show that the entity-enriched BERT model improves effectiveness on entity-oriented queries over a regular BERT model, establishing a new state-of-the-art result for the entity retrieval task, with substantial improvements for complex natural language queries and queries requesting a list of entities with a certain property. Additionally, we show that the entity information provided by our entity-enriched model particularly helps queries related to less popular entities. Last, we observe empirically that the entity-enriched BERT models enable fine-tuning on limited training data, which otherwise would not be feasible due to the known instabilities of BERT in few-sample fine-tuning, thereby contributing to data-efficient training of BERT for entity search.},
	booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	pages = {1455–1465},
	numpages = {11},
	keywords = {entity embeddings, transformers, bert, entity retrieval},
	location = {Madrid, Spain},
	series = {SIGIR '22}
}

@inproceedings{lin-etal-2012-entity,
	title = {{Entity Linking at Web Scale}},
	author = "Lin, Thomas and {Mausam} and Etzioni, Oren",
	booktitle = "Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction ({AKBC}-{WEKEX})",
	month = jun,
	year = "2012",
	pages = "84--88"
}

@inproceedings{doc-ranking-entity,
	author = {Xiong, Chenyan and Callan, Jamie and Liu, Tie-Yan},
	title = {{Word-Entity Duet Representations for Document Ranking}},
	year = {2017},
	isbn = {9781450350228},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	doi = {10.1145/3077136.3080768},
	abstract = {This paper presents a word-entity duet framework for utilizing knowledge bases in ad-hoc retrieval. In this work, the query and documents are modeled by word-based representations and entity-based representations. Ranking features are generated by the interactions between the two representations, incorporating information from the word space, the entity space, and the cross-space connections through the knowledge graph. To handle the uncertainties from the automatically constructed entity representations, an attention-based ranking model AttR-Duet is developed. With back-propagation from ranking labels, the model learns simultaneously how to demote noisy entities and how to rank documents with the word-entity duet. Evaluation results on TREC Web Track ad-hoc task demonstrate that all of the four-way interactions in the duet are useful, the attention mechanism successfully steers the model away from noisy entities, and together they significantly outperform both word-based and entity-based learning to rank systems.},
	booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	pages = {763–772},
	numpages = {10},
	keywords = {text representation, entity-based search, document ranking},
	location = {Shinjuku, Tokyo, Japan},
	series = {SIGIR '17}
}

@inproceedings{query-recommendation-entity,
	author = {Reinanda, Ridho and Meij, Edgar and de Rijke, Maarten},
	title = {{Mining, Ranking and Recommending Entity Aspects}},
	year = {2015},
	isbn = {9781450336215},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	doi = {10.1145/2766462.2767724},
	abstract = {Entity queries constitute a large fraction of web search queries and most of these queries are in the form of an entity mention plus some context terms that represent an intent in the context of that entity. We refer to these entity-oriented search intents as entity aspects. Recognizing entity aspects in a query can improve various search applications such as providing direct answers, diversifying search results, and recommending queries. In this paper we focus on the tasks of identifying, ranking, and recommending entity aspects, and propose an approach that mines, clusters, and ranks such aspects from query logs. We perform large-scale experiments based on users' search sessions from actual query logs to evaluate the aspect ranking and recommendation tasks. In the aspect ranking task, we aim to satisfy most users' entity queries, and evaluate this task in a query-independent fashion. We find that entropy-based methods achieve the best performance compared to maximum likelihood and language modeling approaches. In the aspect recommendation task, we recommend other aspects related to the aspect currently being queried. We propose two approaches based on semantic relatedness and aspect transitions within user sessions and find that a combined approach gives the best performance. As an additional experiment, we utilize entity aspects for actual query recommendation and find that our approach improves the effectiveness of query recommendations built on top of the query-flow graph.},
	booktitle = {Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	pages = {263–272},
	numpages = {10},
	keywords = {semantic search, query intent, entity aspects},
	location = {Santiago, Chile},
	series = {SIGIR '15}
}

@inproceedings{dbpedia-spotlight,
	author = {Mendes, Pablo N. and Jakob, Max and Garc\'{\i}a-Silva, Andr\'{e}s and Bizer, Christian},
	title = {{DBpedia Spotlight: Shedding Light on the Web of Documents}},
	year = {2011},
	booktitle = {Proceedings of the 7th International Conference on Semantic Systems},
	pages = {1–8},
	numpages = {9},
	series = {I-Semantics '11}
}

@inproceedings{tagme,
	author = {Ferragina, Paolo and Scaiella, Ugo},
	title = {{TAGME: On-the-Fly Annotation of Short Text Fragments (by Wikipedia Entities)}},
	year = {2010},
	isbn = {9781450300995},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	doi = {10.1145/1871437.1871689},
	abstract = {We designed and implemented TAGME, a system that is able to efficiently and judiciously augment a plain-text with pertinent hyperlinks to Wikipedia pages. The specialty of TAGME with respect to known systems [5,8] is that it may annotate texts which are short and poorly composed, such as snippets of search-engine results, tweets, news, etc.. This annotation is extremely informative, so any task that is currently addressed using the bag-of-words paradigm could benefit from using this annotation to draw upon (the millions of) Wikipedia pages and their inter-relations.},
	booktitle = {Proceedings of the 19th ACM International Conference on Information and Knowledge Management},
	pages = {1625–1628},
	numpages = {4},
	keywords = {wikipedia, semantic annotation, word sense disambiguation, text mining},
	location = {Toronto, ON, Canada},
	series = {CIKM '10}
}

@inproceedings{wat,
	author = {Piccinno, Francesco and Ferragina, Paolo},
	title = {{From TagME to WAT: A New Entity Annotator}},
	year = {2014},
	isbn = {9781450330237},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	doi = {10.1145/2633211.2634350},
	abstract = {In this paper we propose a novel entity annotator for texts which hinges on TagME's algorithmic technology, currently the best one available. The novelty is twofold: from the one hand, we have engineered the software in order to be modular and more efficient; from the other hand, we have improved the annotation pipeline by re-designing all of its three main modules: spotting, disambiguation and pruning. In particular, the re-design has involved the detailed inspection of the performance of these modules by developing new algorithms which have been in turn tested over all publicly available datasets (i.e. AIDA, IITB, MSN, AQUAINT, and the one of the ERD Challenge). This extensive experimentation allowed us to derive the best combination which achieved on the ERD development dataset an F1 score of 74.8%, which turned to be 67.2% F1 for the test dataset. This final result was due to an impressive precision equal to 87.6%, but very low recall 54.5%. With respect to classic TagME on the development dataset the improvement ranged from 1% to 9% on the D2W benchmark, depending on the disambiguation algorithm being used. As a side result, the final software can be interpreted as a flexible library of several parsing/disambiguation and pruning modules that can be used to build up new and more sophisticated entity annotators. We plan to release our library to the public as an open-source project.},
	booktitle = {Proceedings of the First International Workshop on Entity Recognition & Disambiguation},
	pages = {55–62},
	numpages = {8},
	keywords = {entity annotation, wikipedia, tagme, graph-based algorithms},
	location = {Gold Coast, Queensland, Australia},
	series = {ERD '14}
}

@inproceedings{ED-paper,
	title = {{Improving Entity Linking by Modeling Latent Relations between Mentions}},
	author = "Le, Phong  and
	Titov, Ivan",
	booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
	month = jul,
	year = "2018",
	address = "Melbourne, Australia",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/P18-1148",
	doi = "10.18653/v1/P18-1148",
	pages = "1595--1604",
	abstract = "Entity linking involves aligning textual mentions of named entities to their corresponding entries in a knowledge base. Entity linking systems often exploit relations between textual mentions in a document (e.g., coreference) to decide if the linking decisions are compatible. Unlike previous approaches, which relied on supervised systems or heuristics to predict these relations, we treat relations as latent variables in our neural entity-linking model. We induce the relations without any supervision while optimizing the entity-linking system in an end-to-end fashion. Our multi-relational model achieves the best reported scores on the standard benchmark (AIDA-CoNLL) and substantially outperforms its relation-agnostic version. Its training also converges much faster, suggesting that the injected structural bias helps to explain regularities in the training data.",
}

@inproceedings{nordlys,
	author = {Hasibi, Faegheh and Balog, Krisztian and Garigliotti, Dar\'{\i}o and Zhang, Shuo},
	title = {{Nordlys: A Toolkit for Entity-Oriented and Semantic Search}},
	year = {2017},
	isbn = {9781450350228},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	doi = {10.1145/3077136.3084149},
	abstract = {We introduce Nordlys, a toolkit for entity-oriented and semantic search. It provides functionality for entity cataloging, entity retrieval, entity linking, and target type identification. Nordlys may be used as a Python library or as a RESTful API, and also comes with a web-based user interface. The toolkit is open source and is available at http://nordlys.cc.},
	booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	pages = {1289–1292},
	numpages = {4},
	keywords = {entity retrieval, entity linking, semantic search, result presentation},
	location = {Shinjuku, Tokyo, Japan},
	series = {SIGIR '17}
}

@inproceedings{el-ranking-hasibi,
	author = {Hasibi, Faegheh and Balog, Krisztian and Bratsberg, Svein Erik},
	title = {{Exploiting Entity Linking in Queries for Entity Retrieval}},
	year = {2016},
	isbn = {9781450344975},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	doi = {10.1145/2970398.2970406},
	abstract = {The premise of entity retrieval is to better answer search queries by returning specific entities instead of documents. Many queries mention particular entities; recognizing and linking them to the corresponding entry in a knowledge base is known as the task of entity linking in queries. In this paper we make a first attempt at bringing together these two, i.e., leveraging entity annotations of queries in the entity retrieval model. We introduce a new probabilistic component and show how it can be applied on top of any term-based entity retrieval model that can be emulated in the Markov Random Field framework, including language models, sequential dependence models, as well as their fielded variations. Using a standard entity retrieval test collection, we show that our extension brings consistent improvements over all baseline methods, including the current state-of-the-art. We further show that our extension is robust against parameter settings.},
	booktitle = {Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval},
	pages = {209–218},
	numpages = {10},
	keywords = {entity retrieval, entity linking, semistructured retrieval},
	location = {Newark, Delaware, USA},
	series = {ICTIR '16}
}

@inproceedings{el-balog,
	author = {Balog, Krisztian and Ramampiaro, Heri and Takhirov, Naimdjon and N\o{}rv\r{a}g, Kjetil},
	title = {{Multi-Step Classification Approaches to Cumulative Citation Recommendation}},
	year = {2013},
	isbn = {9782905450098},
	publisher = {Le centre de hautes études internationales d'informatique documentaire},
	address = {Paris, France},
	abstract = {Knowledge bases have become indispensable sources of information. It is therefore critical that they rely on the latest information available and get updated every time new facts surface. Knowledge base acceleration (KBA) systems seek to help humans expand knowledge bases like Wikipedia by automatically recommending edits based on incoming content streams. A core step in this process is that of identifying relevant content, i.e., filtering documents that would imply modifications to the attributes or relations of a given target entity. We propose two multi-step classification approaches for this task that consist of two and three binary classification steps, respectively. Both methods share the same initial component, which is concerned with the identification of entity mentions in documents, while subsequent steps involve identification of documents being relevant and/or central to a given entity. Using the evaluation platform of the TREC 2012 KBA track and a rich feature set developed for this particular task, we show that both approaches deliver state-of-the-art performance.},
	booktitle = {Proceedings of the 10th Conference on Open Research Areas in Information Retrieval},
	pages = {121–128},
	numpages = {8},
	keywords = {information filtering, knowledge base acceleration, cumulative citation recommendation},
	location = {Lisbon, Portugal},
	series = {OAIR '13}
}

@article{watson,
	author = {Ferrucci, D.A.},
	year = {2012},
	month = {05},
	pages = {1:1-1:15},
	title = {{Introduction to ``This is Watson''}},
	volume = {56},
	journal = {IBM Journal of Research and Development},
	doi = {10.1147/JRD.2012.2184356}
}

@inproceedings{yang-etal-2018-collective,
	title = {{Collective Entity Disambiguation with Structured Gradient Tree Boosting}},
	author = "Yang, Yi  and
	Irsoy, Ozan  and
	Rahman, Kazi Shefaet",
	booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
	month = jun,
	year = "2018",
	address = "New Orleans, Louisiana",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/N18-1071",
	doi = "10.18653/v1/N18-1071",
	pages = "777--786",
	abstract = "We present a gradient-tree-boosting-based structured learning model for jointly disambiguating named entities in a document. Gradient tree boosting is a widely used machine learning algorithm that underlies many top-performing natural language processing systems. Surprisingly, most works limit the use of gradient tree boosting as a tool for regular classification or regression problems, despite the structured nature of language. To the best of our knowledge, our work is the first one that employs the structured gradient tree boosting (SGTB) algorithm for collective entity disambiguation. By defining global features over previous disambiguation decisions and jointly modeling them with local features, our system is able to produce globally optimized entity assignments for mentions in a document. Exact inference is prohibitively expensive for our globally normalized model. To solve this problem, we propose Bidirectional Beam Search with Gold path (BiBSG), an approximate inference algorithm that is a variant of the standard beam search algorithm. BiBSG makes use of global information from both past and future to perform better local search. Experiments on standard benchmark datasets show that SGTB significantly improves upon published results. Specifically, SGTB outperforms the previous state-of-the-art neural system by near 1{\%} absolute accuracy on the popular AIDA-CoNLL dataset.",
}

@inproceedings{chatterjee2022bert,
	author = {Chatterjee, Shubham and Dietz, Laura},
	title = {{BERT-ER: Query-Specific BERT Entity Representations for Entity Ranking}},
	year = {2022},
	isbn = {9781450387323},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	doi = {10.1145/3477495.3531944},
	abstract = {Entity-oriented search systems often learn vector representations of entities via the introductory paragraph from the Wikipedia page of the entity. As such representations are the same for every query, our hypothesis is that the representations are not ideal for IR tasks. In this work, we present BERT Entity Representations (BERT-ER) which are query-specific vector representations of entities obtained from text that describes how an entity is relevant for a query. Using BERT-ER in a downstream entity ranking system, we achieve a performance improvement of 13-42% (Mean Average Precision) over a system that uses the BERT embedding of the introductory paragraph from Wikipedia on two large-scale test collections. Our approach also outperforms entity ranking systems using entity embeddings from Wikipedia2Vec, ERNIE, and E-BERT. We show that our entity ranking system using BERT-ER can increase precision at the top of the ranking by promoting relevant entities to the top. With this work, we release our BERT models and query-specific entity embeddings fine-tuned for the entity ranking task.},
	booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	pages = {1466–1477},
	numpages = {12},
	keywords = {bert, entity ranking, query-specific entity representations},
	location = {Madrid, Spain},
	series = {SIGIR '22}
}


@inproceedings{genre,
	author    = {Nicola {De Cao} and
	Gautier Izacard and
	Sebastian Riedel and
	Fabio Petroni},
	title     = {{Autoregressive Entity Retrieval}},
	booktitle = {9th International Conference on Learning Representations, {ICLR} 2021,
	Virtual Event, Austria, May 3-7, 2021},
	publisher = {OpenReview.net},
	year      = {2021},
	url       = {https://openreview.net/forum?id=5k8F6UU39V},
}

@inproceedings{cypher,
	author = {Francis, Nadime and Green, Alastair and Guagliardo, Paolo and Libkin, Leonid and Lindaaker, Tobias and Marsault, Victor and Plantikow, Stefan and Rydberg, Mats and Selmer, Petra and Taylor, Andr\'{e}s},
	title = {{Cypher: An Evolving Query Language for Property Graphs}},
	year = {2018},
	isbn = {9781450347037},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	doi = {10.1145/3183713.3190657},
	abstract = {The Cypher property graph query language is an evolving language, originally designed and implemented as part of the Neo4j graph database, and it is currently used by several commercial database products and researchers. We describe Cypher 9, which is the first version of the language governed by the openCypher Implementers Group. We first introduce the language by example, and describe its uses in industry. We then provide a formal semantic definition of the core read-query features of Cypher, including its variant of the property graph data model, and its ASCII Art graph pattern matching mechanism for expressing subgraphs of interest to an application. We compare the features of Cypher to other property graph query languages, and describe extensions, at an advanced stage of development, which will form part of Cypher 10, turning the language into a compositional language which supports graph projections and multiple named graphs.},
	booktitle = {Proceedings of the 2018 International Conference on Management of Data},
	pages = {1433–1445},
	numpages = {13},
	keywords = {graph databases, formal specification, formal semantics, query language, cypher, property graphs},
	location = {Houston, TX, USA},
	series = {SIGMOD '18}
}

@inproceedings{soboroff2018trec,
	title={{TREC 2018 News Track Overview.}},
	author={Soboroff, Ian and Huang, Shudong and Harman, Donna},
	booktitle={Proceedings of The Twenty-Seventh Text REtrieval Conference},
	series={TREC '18},
	location = {Gaithersburg, Maryland, USA},
	address = {Gaithersburg, Maryland, USA},
	publisher = {National Institute for Standards and Technology (NIST)},
	year={2019},
	numpages={9}
}

@inproceedings{anserini-news,
	title={{Anserini at TREC 2018: Centre, common core, and news tracks}},
	author={Yang, Peilin and Lin, Jimmy},
	booktitle={Proceedings of the Twenty-Seventh Text REtrieval Conference (TREC 2018), Gaithersburg, MD},
	series={TREC '18},
	location = {Gaithersburg, Maryland, USA},
	address = {Gaithersburg, Maryland, USA},
	publisher = {National Institute for Standards and Technology (NIST)},
	year={2019}
}


@inproceedings{contextual-suggestion-track,
	title={{Overview of the TREC 2013 Contextual Suggestion Track}},
	author={Dean-Hall, Adriel and {L.A. Clarke}, Charles and Kamps, Jaap and Thomas, Paul and Simone, Nicole and Voorhees, Ellen},
	booktitle={Proceedings of The Twenty-Second Text REtrieval Conference
	(TREC 2013) Proceedings},
	series={TREC '13},
	location = {Gaithersburg, Maryland, USA},
	address = {Gaithersburg, Maryland, USA},
	publisher = {National Institute for Standards and Technology (NIST)},
	year={2014},
	numpages={13}
}

@inproceedings{ciff,
	author = {Lin, Jimmy and Mackenzie, Joel and Kamphuis, Chris and Macdonald, Craig and Mallia, Antonio and Siedlaczek, Michał and Trotman, Andrew and de Vries, Arjen P.},
	title = {{Supporting Interoperability Between Open-Source Search Engines with the Common Index File Format}},
	year = {2020},
	isbn = {9781450380164},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	doi = {10.1145/3397271.3401404},
	abstract = {There exists a natural tension between encouraging a diverse ecosystem of open-source search engines and supporting fair, replicable comparisons across those systems. To balance these two goals, we examine two approaches to providing interoperability between the inverted indexes of several systems. The first takes advantage of internal abstractions around index structures and building wrappers that allow one system to directly read the indexes of another. The second involves sharing indexes across systems via a data exchange specification that we have developed, called the Common Index File Format (CIFF). We demonstrate the first approach with the Java systems Anserini and Terrier, and the second approach with Anserini, JASSv2, OldDog, PISA, and Terrier. Together, these systems provide a wide range of implementations and features, with different research goals. Overall, we recommend CIFF as a low-effort approach to support independent innovation while enabling the types of fair evaluations that are critical for driving the field forward.},
	booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
	pages = {2149–2152},
	numpages = {4},
	keywords = {inverted indexes, effectiveness and efficiency evaluation, protocol buffers},
	location = {Virtual Event, China},
	series = {SIGIR '20}
}

@inproceedings{ctd,
	author = {Singhal, Amit and Buckley, Chris and Mitra, Mandar},
	title = {Pivoted Document Length Normalization},
	year = {1996},
	isbn = {0897917928},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	doi = {10.1145/243199.243206},
	booktitle = {Proceedings of the 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
	pages = {21–29},
	numpages = {9},
	location = {Zurich, Switzerland},
	series = {SIGIR '96}
}

@article{burstiness_rule,
	title={{Poisson mixtures}},
	volume={1},
	DOI={10.1017/S1351324900000139},
	number={2},
	journal={Natural Language Engineering},
	publisher={Cambridge University Press},
	author={Church, Kenneth W. and Gale, William A.},
	year={1995},
	pages={163–190}
}

@article{fuhr-pra,
	author = {Fuhr, Norbert and R\"{o}lleke, Thomas},
	title = {{A Probabilistic Relational Algebra for the Integration of Information Retrieval and Database Systems}},
	year = {1997},
	issue_date = {Jan. 1997},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {15},
	number = {1},
	issn = {1046-8188},
	doi = {10.1145/239041.239045},
	abstract = {We present a probabilistic relational algebra (PRA) which is a generalization of standard relational algebra. In PRA, tuples are assigned probabilistic weights giving the probability that a tuple belongs to a relation. Based on intensional semantics, the tuple weights of the result of a PRA expression always conform to the underlying probabilistic model. We also show for which expressions extensional semantics yields the same results. Furthermore, we discuss complexity issues and indicate possibilities for optimization. With regard to databases, the approach allows for representing imprecise attribute values, whereas for information retrieval, probabilistic document indexing and probabilistic search term weighting can be modeled. We introduce the concept of vague predicates which yield probabilistic weights instead of Boolean values, thus allowing for queries with vague selection conditions. With these features, PRA implements uncertainty and vagueness in combination with the relational model.},
	journal = {ACM Trans. Inf. Syst.},
	month = {jan},
	pages = {32–66},
	numpages = {35},
	keywords = {logical retrieval model, probabilistic retrieval, relational data model, vague predicates, hypertext retrieval, uncortain data, imprecise data}
}

@article{RSJ,
	author = {Robertson, Stephen E. and {Sp\"{a}rck Jones}, Karen},
	title = {Relevance weighting of search terms},
	journal = {Journal of the American Society for Information Science},
	volume = {27},
	number = {3},
	pages = {129-146},
	doi = {https://doi.org/10.1002/asi.4630270302},
	eprint = {https://asistdl.onlinelibrary.wiley.com/doi/pdf/10.1002/asi.4630270302},
	abstract = {Abstract This paper examines statistical techniques for exploiting relevance information to weight search terms. These techniques are presented as a natural extension of weighting methods using information about the distribution of index terms in documents in general. A series of relevance weighting functions is derived and is justified by theoretical considerations. In particular, it is shown that specific weighted search methods are implied by a general probabilistic theory of retrieval. Different applications of relevance weighting are illustrated by experimental results for test collections.},
	year = {1976}
}

@article{bm25-beyond,
	author = {Robertson, Stephen E. and Zaragoza, Hugo},
	title = {The Probabilistic Relevance Framework: BM25 and Beyond},
	year = {2009},
	issue_date = {April 2009},
	publisher = {Now Publishers Inc.},
	address = {Hanover, MA, USA},
	volume = {3},
	number = {4},
	issn = {1554-0669},
	doi = {10.1561/1500000019},
	abstract = {The Probabilistic Relevance Framework (PRF) is a formal framework for document retrieval, grounded in work done in the 1970—1980s, which led to the development of one of the most successful text-retrieval algorithms, BM25. In recent years, research in the PRF has yielded new retrieval models capable of taking into account document meta-data (especially structure and link-graph information). Again, this has led to one of the most successful Web-search and corporate-search algorithms, BM25F. This work presents the PRF from a conceptual point of view, describing the probabilistic modelling assumptions behind the framework and the different ranking algorithms that result from its application: the binary independence model, relevance feedback models, BM25 and BM25F. It also discusses the relation between the PRF and other statistical models for IR, and covers some related topics, such as the use of non-textual features, and parameter optimisation for models with free parameters.},
	journal = {Found. Trends Inf. Retr.},
	month = {apr},
	pages = {333–389},
	numpages = {57}
}

@inproceedings{pyterrier,
	author = {Macdonald, Craig and Tonellotto, Nicola and MacAvaney, Sean and Ounis, Iadh},
	title = {{PyTerrier: Declarative Experimentation in Python from BM25 to Dense Retrieval}},
	year = {2021},
	isbn = {9781450384469},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	doi = {10.1145/3459637.3482013},
	abstract = {PyTerrier is a Python-based retrieval framework for expressing simple and complex information retrieval (IR) pipelines in a declarative manner. While making use of the long-established Terrier IR platform for basic text indexing and retrieval, its salient utility comes from its expressive Python operators, which allow for individual IR operations to be pipelined and combined in different flexible manners as requested by the search application. Each operation applies a transformation upon a dataframe, while operators are defined with clear semantics in relational algebra. Going further, we have recently expanded the PyTerrier framework to include additional support for state-of-the-art BERT-based text re-rankers (such as EPIC) and dense retrieval implementations (such as ANCE and ColBERT). Transformer pipelines can be tuned and evaluated in a declarative manner. To increase the reusability of this framework as a resource for the IR community, PyTerrier provides easy access to a variety of standard benchmark datasets, including pre-built indices. Finally, we highlight the advantages of such a framework for information retrieval researchers and educators.},
	booktitle = {Proceedings of the 30th ACM International Conference on Information \& Knowledge Management},
	pages = {4526–4533},
	numpages = {8},
	keywords = {neural ranking, experimentation, dense retrieval},
	location = {Virtual Event, Queensland, Australia},
	series = {CIKM '21}
}

@inproceedings{need-graph-db,
	author    = {Chris Kamphuis and Arjen P. de Vries},
	title     = {{Reproducible IR needs an (IR) (graph) query language}},
	booktitle = {Proceedings of the Open-Source {IR} Replicability Challenge co-located
	with 42nd International {ACM} {SIGIR} Conference on Research and Development
	in Information Retrieval, {OSIRRC}@{SIGIR} 2019, Paris, France, July 25,
	2019},
	pages     = {17--20},
	year      = {2019},
	url       = {http://ceur-ws.org/Vol-2409/position03.pdf},
	timestamp = {Fri, 30 Aug 2019 13:15:06 +0200},
	address={Aachen},
	publisher={CEUR-WS.org}
}

@inproceedings{geesedb,
	author    = {Chris Kamphuis and Arjen P. de Vries},
	title     = {{GeeseDB: A Python Graph Engine for Exploration and Search}},
	booktitle = {Proceedings of the 2nd International Conference on Design of Experimental Search \& Information REtrieval Systems},
	pages     = {10-18},
	year      = {2021},
	url       = {http://ceur-ws.org/Vol-2950/paper-11.pdf},
	address={Aachen},
	publisher={CEUR-WS.org},
	series = {DESIRES '21}
}

@inproceedings{rebl,
	author    = {Chris Kamphuis and Faegheh Hasibi and Jimmy Lin and Arjen P. de Vries},
	title     = {{REBL: Entity Linking at Scale}},
	booktitle = {Proceedings of the 3rd International Conference on Design of Experimental Search \& Information REtrieval Systems},
	year      = {2022},
	url       = {https://desires.dei.unipd.it/2022/papers/paper-08.pdf},
	address={Aachen},
	publisher={CEUR-WS.org},
	series = {DESIRES '22}
}

@inproceedings{graphdb-for-ir,
	author="Kamphuis, Chris",
	title={{Graph Databases for Information Retrieval}},
	booktitle="Advances in Information Retrieval",
	year="2020",
	publisher="Springer International Publishing",
	address="Cham",
	pages="608--612",
	abstract="Graph models have been deployed in the context of information retrieval for many years. Computations involving the graph structure are often separated from computations related to the base ranking. In recent years, graph data management has been a topic of interest in database research. We propose to deploy graph database management systems to implement existing and novel graph-based models for information retrieval. For this a unifying mapping from a graph query language to graph based retrieval models needs to be developed; extending standard graph database operations with functionality for keyword search. We also investigate how data structures and algorithms for ranking should change in presence of continuous database updates. We want to investigate how temporal decay can affect ranking when data is continuously updated. Finally, can databases be deployed for efficient two-stage retrieval approaches?",
	isbn="978-3-030-45442-5"
}

@book{neural-ir,
		author = {Lin, Jimmy and Nogueira, Rodrigo and Yates, Andrew},
		title = {Pretrained Transformers for Text Ranking},
		year = {2022},
		subtitle = {BERT and Beyond},
		edition = {1},
		publisher = {Springer Cham},
		isbn = {978-3-031-02181-7},
		pages = {307},
		doi = {https://doi.org/10.1007/978-3-031-02181-7},
}

@inproceedings{trec-2019,
	author={Kamphuis, Chris and Hasibi, Faegheh and de Vries, Arjen P. and Crijns, Tanja},
	title={{Radboud University at TREC 2019}},
	year={2019},
	publisher={[Sl]: NIST},
	booktitle={NIST Special Publication 1250: The Twenty-Eighth Text REtrieval Conference Proceedings (TREC 2019)},
	address={Gaithersburg, Maryland},
	series={TREC '19},
	url={https://trec.nist.gov/pubs/trec28/papers/RUIR.N.C.pdf}
}

@inproceedings{trec-2020,
	author={Boers, Pepijn and Kamphuis, Chris and de Vries, Arjen P.},
	title={{Radboud University at TREC 2020}},
	year={2020},
	publisher={[Sl]: NIST},
	booktitle={NIST Special Publication 1266: The Twenty-Ninth Text REtrieval Conference Proceedings (TREC 2020)},
	address={Gaithersburg, Maryland},
	series = {TREC'20},
	url={https://trec.nist.gov/pubs/trec29/papers/RUIR.N.pdf}
}

@article{trec-covid,
	author    = {Thomas Schoegje and
	Chris Kamphuis and
	Koen Dercksen and
	Djoerd Hiemstra and
	Toine Pieters and
	Arjen P. de Vries},
	title     = {Exploring task-based query expansion at the {TREC-COVID} track},
	journal   = {CoRR},
	volume    = {abs/2010.12674},
	year      = {2020},
	url       = {https://arxiv.org/abs/2010.12674},
	eprinttype = {arXiv},
	eprint    = {2010.12674},
	timestamp = {Mon, 02 Nov 2020 18:17:09 +0100},
	biburl    = {https://dblp.org/rec/journals/corr/abs-2010-12674.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Chaudhuri2005IntegratingDA,
	title={{Integrating DB and IR Technologies: What is the Sound of One Hand Clapping?}},
	author={Surajit Chaudhuri and Raghu Ramakrishnan and Gerhard Weikum},
	title={Proceedings of the Second Biennial Conference on Innovative Data Systems Research},
	series={CIDR'05},
	year={2005},
	location={Asilomar, CA, USA}
}

@article{PowerDB-IR,
	author = {Grabs, Torsten and B\"{o}hm, Klemens and Schek, Hans-J\"{o}rg},
	title = {{PowerDB-IR – Scalable Information Retrieval and Storage with a Cluster of Databases}},
	year = {2004},
	issue_date = {July 2004},
	publisher = {Springer-Verlag},
	address = {Berlin, Heidelberg},
	volume = {6},
	number = {4},
	issn = {0219-1377},
	abstract = {Our objective is a scalable infrastructure for information retrieval (IR) with up-to-date retrieval results in the presence of updates. Timely processing of updates is important with novel application domains such as e-commerce. These issues are challenging, given the additional requirement that the system must scale well. We have built PowerDB-IR, a system that has the characteristics sought. This article describes its design, implementation, and evaluation. We follow a three-tier architecture with a database cluster as the bottom layer for storage management. The rationale for a database cluster is to ‘scale out’, i.e., to add further cluster nodes, whenever necessary for better performance. The middle tier provides IR-specific retrieval and update services. We deploy state-of-the-art middleware software to coordinate the cluster and to invoke IR-specific components. PowerDB-IR extends the middleware layer with service decomposition and parallelisation. PowerDB-IR has the following features: It supports state-of-the-art retrieval models such as vector space retrieval. It allows documents to be inserted and retrieved concurrently and ensures up-to-date retrieval results with almost no overhead. PowerDB-IR ensures the correctness of global concurrency and recovery. Alternative physical data organisation schemes and respective query processing techniques provide adequate performance for different workloads and database sizes. Scaling out the database cluster yields higher throughput and lower response times. We have run extensive experiments with PowerDB-IR using several commercial database systems as well as different middleware products. Further experiments have quantified the effect of transactional guarantees on performance. The main result is that PowerDB-IR shows surprisingly good scalability and low response times.},
	journal = {Knowl. Inf. Syst.},
	month = {jul},
	pages = {465–505},
	numpages = {41},
	keywords = {Transaction management for IR, Concurrent update and retrieval, Information retrieval, Database cluster}
}

@article{array-db,
	author = {Cornacchia, Roberto and H\'{e}man, S\'{a}ndor and Zukowski, Marcin and Vries, Arjen P. and Boncz, Peter},
	title = {{Flexible and Efficient IR Using Array Databases}},
	year = {2008},
	issue_date = {January 2008},
	publisher = {Springer-Verlag},
	address = {Berlin, Heidelberg},
	volume = {17},
	number = {1},
	issn = {1066-8888},
	doi = {10.1007/s00778-007-0071-0},
	abstract = {The Matrix Framework is a recent proposal by Information Retrieval (IR) researchers to flexibly represent information retrieval models and concepts in a single multi-dimensional array framework. We provide computational support for exactly this framework with the array database system SRAM (Sparse Relational Array Mapping), that works on top of a DBMS. Information retrieval models can be specified in its comprehension-based array query language, in a way that directly corresponds to the underlying mathematical formulas. SRAM efficiently stores sparse arrays in (compressed) relational tables and translates and optimizes array queries into relational queries. In this work, we describe a number of array query optimization rules. To demonstrate their effect on text retrieval, we apply them in the TREC TeraByte track (TREC-TB) efficiency task, using the Okapi BM25 model as our example. It turns out that these optimization rules enable SRAM to automatically translate the BM25 array queries into the relational equivalent of inverted list processing including compression, score materialization and quantization, such as employed by custom-built IR systems. The use of the high-performance MonetDB/X100 relational backend, that provides transparent database compression, allows the system to achieve very fast response times with good precision and low resource usage.},
	journal = {The VLDB Journal},
	month = {jan},
	pages = {151–168},
	numpages = {18},
	keywords = {Array databases, Query optimization, Database compression, Information retrieval}
}

@book{modern-information-retrieval,
	title={{Modern information retrieval}},
	author={Baeza-Yates, Ricardo and Ribeiro-Neto, Berthier and others},
	volume={463},
	year={1999},
	publisher={ACM press New York}
}

@inproceedings{risc,
	author = {Chaudhuri, Surajit and Weikum, Gerhard},
	title = {{Rethinking Database System Architecture: Towards a Self-Tuning RISC-Style Database System}},
	year = {2000},
	isbn = {1558607153},
	publisher = {Morgan Kaufmann Publishers Inc.},
	address = {San Francisco, CA, USA},
	booktitle = {Proceedings of the 26th International Conference on Very Large Data Bases},
	pages = {1–10},
	numpages = {10},
	series = {VLDB '00}
}

@inproceedings{handwritten,
	author={S\'andor H\'eman and Marcin Zukowski and Arjen P. de Vries and Peter Boncz},
	title={{MonetDB/X100 at the 2006 TREC TeraByte Track}},
	year={2006},
	publisher={[Sl]: NIST},
	booktitle={NIST Special Publication: SP 500-272. The Fifteenth Text REtrieval Conference (TREC 2006) Proceedings},
	address={Gaithersburg, Maryland, USA},
	series = {TREC'06},
	url={https://trec.nist.gov/pubs/trec15/papers/cwi-heman.tera.final.pdf}
}

@inproceedings{weak-baselines,
	author = {Yang, Wei and Lu, Kuang and Yang, Peilin and Lin, Jimmy},
	title = {{Critically Examining the "Neural Hype": Weak Baselines and the Additivity of Effectiveness Gains from Neural Ranking Models}},
	year = {2019},
	isbn = {9781450361729},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	doi = {10.1145/3331184.3331340},
	abstract = {Is neural IR mostly hype? In a recent SIGIR Forum article, Lin expressed skepticism that neural ranking models were actually improving ad hoc retrieval effectiveness in limited data scenarios. He provided anecdotal evidence that authors of neural IR papers demonstrate "wins" by comparing against weak baselines. This paper provides a rigorous evaluation of those claims in two ways: First, we conducted a meta-analysis of papers that have reported experimental results on the TREC Robust04 test collection. We do not find evidence of an upward trend in effectiveness over time. In fact, the best reported results are from a decade ago and no recent neural approach comes close. Second, we applied five recent neural models to rerank the strong baselines that Lin used to make his arguments. A significant improvement was observed for one of the models, demonstrating additivity in gains. While there appears to be merit to neural IR approaches, at least some of the gains reported in the literature appear illusory.},
	booktitle = {Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval},
	pages = {1129–1132},
	numpages = {4},
	keywords = {document ranking, neural IR, meta-analysis},
	location = {Paris, France},
	series = {SIGIR'19}
}

@inproceedings{monetdb/x100,
	author = {Boncz, Peter A. and Zukowski, Marcin and Nes, Niels},
	title = {{MonetDB/X100: Hyper-Pipelining Query Execution}},
	pages = {225-237},
	publisher = {www.cidrdb.org},
	year = {2005},
	booktitle = {Proceedings of the Second Biennial Conference on Innovative Data Systems Research},
	location = {Asilomar, CA, USA},
	series = {CIDR'05}
}

@phdthesis{monet,
	author  = {Peter Alexander Boncz},
	title   = {{A Next-Generation DBMS Kernel For Query-Intensive Applications}},
	school  = {University of Amsterdam},
	year    = {2002},
	month   = {May},
}

@inproceedings{vectorwise,
	author = {Zukowski, Marcin and van de Wiel, Mark and Boncz, Peter},
	title = {{Vectorwise: A Vectorized Analytical DBMS}},
	year = {2012},
	isbn = {9780769547473},
	publisher = {IEEE Computer Society},
	address = {USA},
	doi = {10.1109/ICDE.2012.148},
	abstract = {Vector wise is a new entrant in the analytical database marketplace whose technology comes straight from innovations in the database research community in the past years. The product has since made waves due to its excellent performance in analytical customer workloads as well as benchmarks. We describe the history of Vector wise, as well as its basic architecture and the experiences in turning a technology developed in an academic context into a commercial-grade product. Finally, we turn our attention to recent performance results, most notably on the TPC-H benchmark at various sizes.},
	booktitle = {Proceedings of the 2012 IEEE 28th International Conference on Data Engineering},
	pages = {1349–1350},
	numpages = {2},
	series = {ICDE '12}
}

@inproceedings{indri,
	title={{Indri: A language model-based search engine for complex queries}},
	author={Strohman, Trevor and Metzler, Donald and Turtle, Howard and Croft, W. Bruce},
	booktitle={Proceedings of the International Conference on Intelligent Analysis},
	number={6},
	pages={2--6},
	year={2005},
	series={ICIA'05},
	organization={Washington, DC.},
	url={http://ciir.cs.umass.edu/pubfiles/ir-407.pdf}
}

@misc{lucene,
	author = {{Apache Software Foundation}},
	title = {Lucene},
	url = {https://lucene.apache.org/core/4\_3\_0/},
	version = {4.3},
	year = {2013},
}

@inproceedings{MG4J,
	title = {{MG4J at TREC 2005}},
	author="Paolo Boldi and Sebastiano Vigna",
	year = 2005,
	booktitle = "The Fourteenth Text REtrieval Conference (TREC 2005) Proceedings",
	publisher = "NIST",
	series = "Special Papers",
	number = "SP 500-266",
	url = "http://mg4j.di.unimi.it/",
}

@techreport{pagerank,
	number = {1999-66},
	month = {November},
	author = {Lawrence Page and Sergey Brin and Rajeev Motwani and Terry Winograd},
	note = {Previous number = SIDL-WP-1999-0120},
	title = {{The PageRank Citation Ranking: Bringing Order to the Web.}},
	type = {Technical Report},
	publisher = {Stanford InfoLab},
	year = {1999},
	institution = {Stanford InfoLab},
	url = {http://ilpubs.stanford.edu:8090/422/},
	abstract = {The importance of a Web page is an inherently subjective matter, which depends on the readers interests, knowledge and attitudes. But there is still much that can be said objectively about the relative importance of Web pages. This paper describes PageRank, a mathod for rating Web pages objectively and mechanically, effectively measuring the human interest and attention devoted to them. We compare PageRank to an idealized random Web surfer. We show how to efficiently compute PageRank for large numbers of pages. And, we show how to apply PageRank to search and to user navigation.}
}

@inproceedings{scann,
	title={{Accelerating Large-Scale Inference with Anisotropic Vector Quantization}},
	author={Guo, Ruiqi and Sun, Philip and Lindgren, Erik and Geng, Quan and Simcha, David and Chern, Felix and Kumar, Sanjiv},
	booktitle={International Conference on Machine Learning},
	year={2020},
	URL={https://arxiv.org/abs/1908.10396}
}

@article{faiss,
	title={{Billion-scale similarity search with GPUs}},
	author={Johnson, Jeff and Douze, Matthijs and J{\'e}gou, Herv{\'e}},
	journal={IEEE Transactions on Big Data},
	volume={7},
	number={3},
	pages={535--547},
	year={2019},
	publisher={IEEE}
}

@article{seperation-logical-physical,
	author = {Lin, Jimmy},
	title = {{A Proposed Conceptual Framework for a Representational Approach to Information Retrieval}},
	year = {2022},
	issue_date = {December 2021},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {55},
	number = {2},
	issn = {0163-5840},
	url = {https://doi.org/10.1145/3527546.3527552},
	abstract = {This paper outlines a conceptual framework for understanding recent developments in information retrieval and natural language processing that attempts to integrate dense and sparse retrieval methods. I propose a representational approach that breaks the core text retrieval problem into a logical scoring model and a physical retrieval model. The scoring model is defined in terms of encoders, which map queries and documents into a representational space, and a comparison function that computes query-document scores. The physical retrieval model defines how a system produces the top-k scoring documents from an arbitrarily large corpus with respect to a query. The scoring model can be further analyzed along two dimensions: dense vs. sparse representations and supervised (learned) vs. unsupervised approaches. I show that many recently proposed retrieval methods, including multi-stage ranking designs, can be seen as different parameterizations in this framework, and that a unified view suggests a number of open research questions, providing a roadmap for future work. As a bonus, this conceptual framework establishes connections to sentence similarity tasks in natural language processing and information access "technologies" prior to the dawn of computing.},
	journal = {SIGIR Forum},
	month = {mar},
	articleno = {4},
	numpages = {29}
}

@article{thakur2022domain,
	title={{Domain adaptation for memory-efficient dense retrieval}},
	author={Thakur, Nandan and Reimers, Nils and Lin, Jimmy},
	journal={arXiv preprint arXiv:2205.11498},
	year={2022}
}

@inproceedings{sciavolino:2021:simple,
	title = {{Simple Entity-Centric Questions Challenge Dense Retrievers}},
	author = "Sciavolino, Christopher  and
	Zhong, Zexuan  and
	Lee, Jinhyuk  and
	Chen, Danqi",
	booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
	series = "EMNLP '21",
	month = nov,
	year = "2021",
	address = "Online and Punta Cana, Dominican Republic",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2021.emnlp-main.496",
	pages = "6138--6148",
	abstract = "Open-domain question answering has exploded in popularity recently due to the success of dense retrieval models, which have surpassed sparse models using only a few supervised training examples. However, in this paper, we demonstrate current dense models are not yet the holy grail of retrieval. We first construct EntityQuestions, a set of simple, entity-rich questions based on facts from Wikidata (e.g., {``}Where was Arve Furset born?{''}), and observe that dense retrievers drastically under-perform sparse methods. We investigate this issue and uncover that dense retrievers can only generalize to common entities unless the question pattern is explicitly observed during training. We discuss two simple solutions towards addressing this critical problem. First, we demonstrate that data augmentation is unable to fix the generalization problem. Second, we argue a more robust passage encoder helps facilitate better question adaptation using specialized question encoders. We hope our work can shed light on the challenges in creating a robust, universal dense retriever that works well across different input distributions.",
}

@inproceedings{dalton2021cast,
	title={{CAsT 2020: The Conversational Assistance Track Overview}},
	author={Dalton, Jeffrey and Xiong, Chenyan and Callan, Jamie},
	year={2021},
	booktitle={The Twenty-Ninth Text REtrieval Conference (TREC 2020) Proceedings},
	publisher={NIST},
	address={Gaithersburg, Maryland, USA},
	institution={Technical report, Technical report}
}

@inproceedings{bosselut2021dynamic,
	title={{Dynamic Neuro-Symbolic Knowledge Graph Construction for Zero-shot Commonsense Question Answering}},
	author={Bosselut, Antoine and Le Bras, Ronan and Choi, Yejin},
	booktitle={Proceedings of The Thirty-Fifth AAAI Conference on Artificial Intelligence},
	series={AAAI '20},
	volume={35},
	pages={4923--4931},
	year={2021}
}

@inproceedings{
	thakur2021beir,
	title={{BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models}},
	author={Nandan Thakur and Nils Reimers and Andreas R{\"u}ckl{\'e} and Abhishek Srivastava and Iryna Gurevych},
	booktitle={Proceedings of the Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)},
	series={NeurIPS '21},
	year={2021},
	url={https://openreview.net/forum?id=wCu6T5xFjeJ}
}

@inproceedings{xu2022laprador,
	title = {{LaPraDoR: Unsupervised Pretrained Dense Retriever for Zero-Shot Text Retrieval}},
	author = "Xu, Canwen  and
	Guo, Daya  and
	Duan, Nan  and
	McAuley, Julian",
	booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
	month = may,
	year = "2022",
	address = "Dublin, Ireland",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.findings-acl.281",
	pages = "3557--3569",
	abstract = "In this paper, we propose LaPraDoR, a pretrained dual-tower dense retriever that does not require any supervised data for training. Specifically, we first present Iterative Contrastive Learning (ICoL) that iteratively trains the query and document encoders with a cache mechanism. ICoL not only enlarges the number of negative instances but also keeps representations of cached examples in the same hidden space. We then propose Lexicon-Enhanced Dense Retrieval (LEDR) as a simple yet effective way to enhance dense retrieval with lexical matching. We evaluate LaPraDoR on the recently proposed BEIR benchmark, including 18 datasets of 9 zero-shot text retrieval tasks. Experimental results show that LaPraDoR achieves state-of-the-art performance compared with supervised dense retrieval models, and further analysis reveals the effectiveness of our training strategy and objectives. Compared to re-ranking, our lexicon-enhanced approach can be run in milliseconds (22.5x faster) while achieving superior performance.",
}

@inproceedings{Tran:2022:DRE,
	author = {Tran, Hai Dang and Yates, Andrew},
	title = {{Dense Retrieval with Entity Views}},
	year = {2022},
	isbn = {9781450392365},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3511808.3557285},
	abstract = {Pre-trained language models like BERT have been demonstrated to be both effective and efficient ranking methods when combined with approximate nearest neighbor search, which can quickly match dense representations of queries and documents. However, pretrained language models alone do not fully capture information about uncommon entities. In this work, we investigate methods for enriching dense query and document representations with entity information from an external source. Our proposed method identifies groups of entities in a text and encodes them into a dense vector representation, which is then used to enrich BERT's vector representation of the text. To handle documents that contain many loosely-related entities, we devise a strategy for creating multiple entity representations that reflect different views of a document. For example, a document about a scientist may cover aspects of her personal life and recent work, which correspond to different views of the entity. In an evaluation on MS MARCO benchmarks, we find that enriching query and document representations in this way yields substantial increases in effectiveness.},
	booktitle = {Proceedings of the 31st ACM International Conference on Information \& Knowledge Management},
	pages = {1955–1964},
	numpages = {10},
	keywords = {entity representations, ad hoc ranking, dense retrieval},
	series = {CIKM '22}
}

@inproceedings{rel,
	author = {van Hulst, Johannes M. and Hasibi, Faegheh and Dercksen, Koen and Balog, Krisztian and de Vries, Arjen P.},
	title = {{REL: An Entity Linker Standing on the Shoulders of Giants}},
	year = {2020},
	isbn = {9781450380164},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3397271.3401416},
	booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
	pages = {2197–2200},
	numpages = {4},
	keywords = {toolkit, entity linking, entity disambiguation, NER},
	series = {SIGIR '20}
}

@inproceedings{BERT,
	title = {{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}},
	author = "Devlin, Jacob  and
	Chang, Ming-Wei  and
	Lee, Kenton  and
	Toutanova, Kristina",
	booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
	series = {NAACL '19},
	month = jun,
	year = "2019",
	address = "Minneapolis, Minnesota",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/N19-1423",
	doi = "10.18653/v1/N19-1423",
	pages = "4171--4186",
	abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@inproceedings{trec-dl,
	author = {Craswell, Nick and Mitra, Bhaskar and Yilmaz, Emine and Campos, Daniel and Voorhees, Ellen M. and Soboroff, Ian},
	title = {{TREC Deep Learning Track: Reusable Test Collections in the Large Data Regime}},
	year = {2021},
	isbn = {9781450380379},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3404835.3463249},
	abstract = {The TREC Deep Learning (DL) Track studies ad hoc search in the large data regime, meaning that a large set of human-labeled training data is available. Results so far indicate that the best models with large data may be deep neural networks. This paper supports the reuse of the TREC DL test collections in three ways. First we describe the data sets in detail, documenting clearly and in one place some details that are otherwise scattered in track guidelines, overview papers and in our associated MS MARCO leaderboard pages. We intend this description to make it easy for newcomers to use the TREC DL data. Second, because there is some risk of iteration and selection bias when reusing a data set, we describe the best practices for writing a paper using TREC DL data, without overfitting. We provide some illustrative analysis. Finally we address a number of issues around the TREC DL data, including an analysis of reusability.},
	booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	pages = {2369–2375},
	numpages = {7},
	keywords = {reusable benchmark, IR evaluation, deep learning},
	series = {SIGIR '21}
}

@inproceedings{blink,
	title = {{Scalable Zero-shot Entity Linking with Dense Entity Retrieval}},
	author = "Wu, Ledell  and
	Petroni, Fabio  and
	Josifoski, Martin  and
	Riedel, Sebastian  and
	Zettlemoyer, Luke",
	booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
	series={EMNLP '20},
	month = nov,
	year = "2020",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2020.emnlp-main.519",
	pages = "6397--6407",
	abstract = "This paper introduces a conceptually simple, scalable, and highly effective BERT-based entity linking model, along with an extensive evaluation of its accuracy-speed trade-off. We present a two-stage zero-shot linking algorithm, where each entity is defined only by a short textual description. The first stage does retrieval in a dense space defined by a bi-encoder that independently embeds the mention context and the entity descriptions. Each candidate is then re-ranked with a cross-encoder, that concatenates the mention and entity text. Experiments demonstrate that this approach is state of the art on recent zero-shot benchmarks (6 point absolute gains) and also on more established non-zero-shot evaluations (e.g. TACKBP-2010), despite its relative simplicity (e.g. no explicit entity embeddings or manually engineered mention tables). We also show that bi-encoder linking is very fast with nearest neighbor search (e.g. linking with 5.9 million candidates in 2 milliseconds), and that much of the accuracy gain from the more expensive cross-encoder can be transferred to the bi-encoder via knowledge distillation. Our code and models are available at https://github.com/facebookresearch/BLINK.",
}

@inproceedings{flair,
	title = {{FLAIR: An Easy-to-Use Framework for State-of-the-Art NLP}},
	author = "Akbik, Alan  and
	Bergmann, Tanja  and
	Blythe, Duncan  and
	Rasul, Kashif  and
	Schweter, Stefan  and
	Vollgraf, Roland",
	booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics (Demonstrations)",
	series = {NAACL '19},
	month = jun,
	year = "2019",
	address = "Minneapolis, Minnesota",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/N19-4010",
	doi = "10.18653/v1/N19-4010",
	pages = "54--59",
	abstract = "We present FLAIR, an NLP framework designed to facilitate training and distribution of state-of-the-art sequence labeling, text classification and language models. The core idea of the framework is to present a simple, unified interface for conceptually very different types of word and document embeddings. This effectively hides all embedding-specific engineering complexity and allows researchers to {``}mix and match{''} various embeddings with little effort. The framework also implements standard model training and hyperparameter selection routines, as well as a data fetching module that can download publicly available NLP datasets and convert them into data structures for quick set up of experiments. Finally, FLAIR also ships with a {``}model zoo{''} of pre-trained models to allow researchers to use state-of-the-art NLP models in their applications. This paper gives an overview of the framework and its functionality. The framework is available on GitHub at https://github.com/zalandoresearch/flair .",
}

@inproceedings{crosswiki,
	title = {{A Cross-Lingual Dictionary for English Wikipedia Concepts}},
	author = "Spitkovsky, Valentin I.  and
	Chang, Angel X.",
	booktitle = "Proceedings of the Eighth International Conference on Language Resources and Evaluation",
	series = {LREC '12},
	month = may,
	year = "2012",
	address = "Istanbul, Turkey",
	publisher = "European Language Resources Association (ELRA)",
	url = "http://www.lrec-conf.org/proceedings/lrec2012/pdf/266_Paper.pdf",
	pages = "3168--3175",
	abstract = "We present a resource for automatically associating strings of text with English Wikipedia concepts. Our machinery is bi-directional, in the sense that it uses the same fundamental probabilistic methods to map strings to empirical distributions over Wikipedia articles as it does to map article URLs to distributions over short, language-independent strings of natural language text. For maximal inter-operability, we release our resource as a set of flat line-based text files, lexicographically sorted and encoded with UTF-8. These files capture joint probability distributions underlying concepts (we use the terms article, concept and Wikipedia URL interchangeably) and associated snippets of text, as well as other features that can come in handy when working with Wikipedia articles and related information.",
}

@inproceedings{yago,
	title = {{Robust Disambiguation of Named Entities in Text}},
	author = {Hoffart, Johannes  and
	Yosef, Mohamed Amir  and
	Bordino, Ilaria  and
	F{\"u}rstenau, Hagen  and
	Pinkal, Manfred  and
	Spaniol, Marc  and
	Taneva, Bilyana  and
	Thater, Stefan  and
	Weikum, Gerhard},
	booktitle = "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing",
	series = "EMNLP '11",
	month = jul,
	year = "2011",
	address = "Edinburgh, Scotland, UK.",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/D11-1072",
	pages = "782--792",
}

@inproceedings{wikipedia2vec,
	title = {{Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation}},
	author = "Yamada, Ikuya  and
	Shindo, Hiroyuki  and
	Takeda, Hideaki  and
	Takefuji, Yoshiyasu",
	booktitle = "Proceedings of the 20th {SIGNLL} Conference on Computational Natural Language Learning",
	month = aug,
	year = "2016",
	address = "Berlin, Germany",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/K16-1025",
	pages = "250--259",
}

@inproceedings{poly-encoders,
	title={{Poly-encoders: Transformer Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring}},
	author={Humeau, Samuel and Shuster, Kurt and Lachaux, Marie-Anne and Weston, Jason},
	year={2019},
	booktitle={Proceedings of the 7th  International Conference on Learning Representations},
	month={may},
	address={New Orleans, LA, USA},
	url={https://openreview.net/pdf?id=SkxgnnNFvH},
	series={ICLR'19}
}

@inproceedings{10.1145/3459637.3482159,
	author = {Arabzadeh, Negar and Yan, Xinyi and Clarke, Charles L. A.},
	title = {{Predicting Efficiency/Effectiveness Trade-Offs for Dense vs. Sparse Retrieval Strategy Selection}},
	year = {2021},
	isbn = {9781450384469},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3459637.3482159},
	abstract = {Over the last few years, contextualized pre-trained transformer models such as BERT have provided substantial improvements on information retrieval tasks. Traditional sparse retrieval methods such as BM25 rely on high-dimensional, sparse, bag-of-words query representations to retrieve documents. On the other hand, recent approaches based on pre-trained transformer models such as BERT, fine-tune dense low-dimensional contextualized representations of queries and documents in embedding space. While these dense retrievers enjoy substantial retrieval effectiveness improvements compared to sparse retrievers, they are computationally intensive, requiring substantial GPU resources, and dense retrievers are known to be more expensive from both time and resource perspectives. In addition, sparse retrievers have been shown to retrieve complementary information with respect to dense retrievers, leading to proposals for hybrid retrievers. These hybrid retrievers leverage low-cost, exact-matching based sparse retrievers along with dense retrievers to bridge the semantic gaps between query and documents. In this work, we address this trade-off between the cost and utility of sparse vs dense retrievers by proposing a classifier to select a suitable retrieval strategy (i.e., sparse vs. dense vs. hybrid) for individual queries. Leveraging sparse retrievers for queries which can be answered with sparse retrievers decreases the number of calls to GPUs. Consequently, while utility is maintained, query latency decreases. Although we use less computational resources and spend less time, we still achieve improved performance. Our classifier can select between sparse and dense retrieval strategies based on the query alone. We conduct experiments on the MS MARCO passage dataset demonstrating an improved range of efficiency/effectiveness trade-offs between purely sparse, purely dense or hybrid retrieval strategies, allowing an appropriate strategy to be selected based on a target latency and resource budget.},
	booktitle = {Proceedings of the 30th ACM International Conference on Information and Knowledge Management},
	pages = {2862–2866},
	numpages = {5},
	keywords = {efficiency, query latency, dense retriever, sparse retriever},
	series = {CIKM '21}
}

@inproceedings{10.1145/1571941.1572114,
	author = {Cormack, Gordon V. and Clarke, Charles L A and Buettcher, Stefan},
	title = {{Reciprocal Rank Fusion Outperforms Condorcet and Individual Rank Learning Methods}},
	year = {2009},
	isbn = {9781605584836},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/1571941.1572114},
	abstract = {Reciprocal Rank Fusion (RRF), a simple method for combining the document rankings from multiple IR systems, consistently yields better results than any individual system, and better results than the standard method Condorcet Fuse. This result is demonstrated by using RRF to combine the results of several TREC experiments, and to build a meta-learner that ranks the LETOR 3 dataset better than any previously reported method},
	booktitle = {Proceedings of the 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval},
	pages = {758–759},
	numpages = {2},
	keywords = {aggregation, fusion, ranking},
	series = {SIGIR '09}
}

@article{anserini,
	author = {Yang, Peilin and Fang, Hui and Lin, Jimmy},
	title = {{Anserini: Reproducible Ranking Baselines Using Lucene}},
	year = {2018},
	issue_date = {December 2018},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {10},
	number = {4},
	issn = {1936-1955},
	url = {https://doi.org/10.1145/3239571},
	abstract = {This work tackles the perennial problem of reproducible baselines in information retrieval research, focusing on bag-of-words ranking models. Although academic information retrieval researchers have a long history of building and sharing systems, they are primarily designed to facilitate the publication of research papers. As such, these systems are often incomplete, inflexible, poorly documented, difficult to use, and slow, particularly in the context of modern web-scale collections. Furthermore, the growing complexity of modern software ecosystems and the resource constraints most academic research groups operate under make maintaining open-source systems a constant struggle. However, except for a small number of companies (mostly commercial web search engines) that deploy custom infrastructure, Lucene has become the de facto platform in industry for building search applications. Lucene has an active developer base, a large audience of users, and diverse capabilities to work with heterogeneous collections at scale. However, it lacks systematic support for ad hoc experimentation using standard test collections. We describe Anserini, an information retrieval toolkit built on Lucene that fills this gap. Our goal is to simplify ad hoc experimentation and allow researchers to easily reproduce results with modern bag-of-words ranking models on diverse test collections. With Anserini, we demonstrate that Lucene provides a suitable framework for supporting information retrieval research. Experiments show that our system efficiently indexes large web collections, provides modern ranking models that are on par with research implementations in terms of effectiveness, and supports low-latency query evaluation to facilitate rapid experimentation},
	journal = {Journal of Data and Information Quality},
	month_ = {oct},
	articleno = {16},
	numpages = {20},
	keywords = {Ad hoc retrieval, TREC}
}

@mastersthesis{Shehata,
	author={Shehata, Dahlia},
	title={{Information Retrieval with Entity Linking}},
	school={University of Waterloo},
	year={2022},
	publisher="UWSpace",
	url={http://hdl.handle.net/10012/18557}
}

@inproceedings{chameleons,
	author = {Arabzadeh, Negar and Mitra, Bhaskar and Bagheri, Ebrahim},
	title = {{MS MARCO Chameleons: Challenging the MS MARCO Leaderboard with Extremely Obstinate Queries}},
	year = {2021},
	isbn = {9781450384469},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3459637.3482011},
	abstract = {During the recent years and with the growing influence of neural architectures, tasks such as ad hoc retrieval have witnessed an impressive improvement in performance. For instance, the performance of rankers on the passage retrieval task on the MS MARCO dataset has improved by an order of magnitude in less than two years. In this paper, we go beyond the overall performance of the state of the art rankers and empirically study their performance from a finer-grained perspective. We find that while neural rankers have been able to consistently improve performance, this has been in part thanks to a specific set of queries from within the larger query set. We systematically show that there are subsets of queries that are difficult for each and every one of the neural rankers, which we refer to as obstinate queries. We show the obstinate queries are similar to easier queries in terms of their number of available relevant judgement documents and the length of the query itself but they are extremely more difficult to satisfy by existing rankers. Furthermore, we observe that query reformulation methods cannot help these queries. On this basis, we present three datasets derived from the MS MARCO Dev set, called the MS MARCO Chameleon datasets. We believe that the next breakthrough in performance would need to necessarily consider the queries in the MS MARCO Chameleons, as such, propose that a well-rounded evaluation strategy for any new ranker would need to include performance measures on both the overall MS MARCO dataset as well as the proposed MS MARCO Chameleon datasets.},
	booktitle = {Proceedings of the 30th ACM International Conference on Information \& Knowledge Management},
	pages = {4426–4435},
	numpages = {10},
	keywords = {query difficulty, query reformulation, information retrieval},
	series = {CIKM '21}
}

@article{fuhr-mrr,
	author = {Fuhr, Norbert},
	title = {{Some Common Mistakes In IR Evaluation, And How They Can Be Avoided}},
	year = {2018},
	issue_date = {December 2017},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {51},
	number = {3},
	issn = {0163-5840},
	url = {https://doi.org/10.1145/3190580.3190586},
	abstract = {This paper points out some mistakes that can be frequently found in IR publications: MRR and ERR violate basic requirements for a metric, MAP is based on unrealistic assumptions, the numbers shown overstate the precision of the result, relative improvements of arithmetic means are inappropriate, the simple holdout method yields unreliable results, hypotheses are often formulated after the experiment, significance tests frequently ignore the multiple comparisons problem, effect sizes are ignored, reproducibility of the experiments might be nearly impossible, and sometimes authors claim proof by experimentation.},
	journal = {SIGIR Forum},
	month_ = {feb},
	pages = {32–41},
	numpages = {10}
}

@inproceedings{qlever,
	author = {Bast, Hannah and Buchhold, Bj\"{o}rn},
	title = {{QLever: A Query Engine for Efficient SPARQL+Text Search}},
	year = {2017},
	isbn = {9781450349185},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	abstract = {We present QLever, a query engine for efficient combined search on a knowledge base and a text corpus, in which named entities from the knowledge base have been identified (that is, recognized and disambiguated). The query language is SPARQL extended by two QLever-specific predicates ql:contains-entity and ql:contains-word, which can express the occurrence of an entity or word (the object of the predicate) in a text record (the subject of the predicate). We evaluate QLever on two large datasets, including FACC (the ClueWeb12 corpus linked to Freebase). We compare against three state-of-the-art query engines for knowledge bases with varying support for text search: RDF-3X, Virtuoso, Broccoli. Query times are competitive and often faster on the pure SPARQL queries, and several orders of magnitude faster on the SPARQL+Text queries. Index size is larger for pure SPARQL queries, but smaller for SPARQL+Text queries.},
	booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
	pages = {647–656},
	numpages = {10},
	keywords = {indexing, sparql+text, efficiency},
	series = {CIKM '17}
}

@article{2020t5,
	author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
	title   = {{Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer}},
	journal = {Journal of Machine Learning Research},
	year    = {2020},
	volume  = {21},
	number  = {140},
	pages   = {1-67},
	url     = {http://jmlr.org/papers/v21/20-074.html}
}

@techreport{lambdamart,
	added-at = {2013-01-12T16:27:53.000+0100},
	author = {Burges, Christopher J. C.},
	biburl = {https://www.bibsonomy.org/bibtex/2890499272759d00d48b45aa8826018fb/nosebrain},
	institution = {Microsoft Research},
	keywords = {LambdaMART LambdaRank RankNet learning ranking},
	timestamp = {2013-01-12T16:27:53.000+0100},
	title = {{From RankNet to LambdaRank to LambdaMART: An Overview}},
	url = {http://research.microsoft.com/en-us/um/people/cburges/tech\_reports/MSR-TR-2010-82.pdf},
	year = 2010
}

@inproceedings{mmead,
	author = {Kamphuis, Chris and Lin, Aileen and Yang, Siwen and Lin, Jimmy and de Vries, Arjen P. and Hasibi, Faegheh},
	title = {{MMEAD: MS MARCO Entity Annotations and Disambiguations}},
	year = {2023},
	isbn = {978-1-4503-9408-6},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	doi = {10.1145/3539618.3591887},
	abstract = {MMEAD, or MS MARCO Entity Annotations and Disambiguations, is a resource for entity links for the MS MARCO datasets. We specify a format to store and share links for both document and passage collections of MS MARCO. Following this specification, we release entity links to Wikipedia for documents and passages in both MS MARCO collections (v1 and v2). Entity links have been produced by the REL and BLINK systems. MMEAD is an easy-to-install Python package, allowing users to load the link data and entity embeddings effortlessly. Using MMEAD takes only a few lines of code. Finally, we show how MMEAD can be used for IR research that uses entity information. We show how to improve recall@1000 and MRR@10 on more complex queries on the MS MARCO v1 passage dataset by using this resource. We also demonstrate how entity expansions can be used for interactive search applications.},
	booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	numpages = {9},
	keywords = {Information Retrieval, Entity Linking},
	location = {Taipei, Taiwan},
	series = {SIGIR '23}
}

@article{VectorSpaceModel,
	author = {Salton, G. and Wong, A. and Yang, C. S.},
	title = {{A Vector Space Model for Automatic Indexing}},
	year = {1975},
	issue_date = {Nov. 1975},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {18},
	number = {11},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/361219.361220},
	doi = {10.1145/361219.361220},
	abstract = {In a document retrieval, or other pattern matching environment where stored entities (documents) are compared with each other or with incoming patterns (search requests), it appears that the best indexing (property) space is one where each entity lies as far away from the others as possible; in these circumstances the value of an indexing system may be expressible as a function of the density of the object space; in particular, retrieval performance may correlate inversely with space density. An approach based on space density computations is used to choose an optimum indexing vocabulary for a collection of documents. Typical evaluation results are shown, demonstating the usefulness of the model.},
	journal = {Communications of the ACM},
	month = {nov},
	pages = {613–620},
	numpages = {8},
	keywords = {document space, content analysis, automatic indexing, automatic information retrieval}
}

@article{idf,
	title={{A Statistical Interpretation of Term Specificity and its Application in Retrieval}},
	author={Sp{\"a}rck Jones, Karen},
	journal={Journal of Documentation},
	year={1972},
	volume={60},
	pages={493-502}
}

@inproceedings{croft_lm,
	author = {Song, Fei and Croft, W. Bruce},
	title = {A General Language Model for Information Retrieval},
	year = {1999},
	isbn = {1581131461},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/319950.320022},
	doi = {10.1145/319950.320022},
	abstract = {Statistical language modeling has been successfully used for speech recognition, part-of-speech tagging, and syntactic parsing. Recently, it has also been applied to information retrieval. According to this new paradigm, each document is viewed as a language sample, and a query as a generation process. The retrieved documents are ranked based on the probabilities of producing a query from the corresponding language models of these documents. In this paper, we will present a new language model for information retrieval, which is based on a range of data smoothing techniques, including the Good-Turning estimate, curve-fitting functions, and model combinations. Our model is conceptually simple and intuitive, and can be easily extended to incorporate probabilities of phrases such as word pairs and word triples. The experiments with the Wall Street Journal and TREC4 data sets showed that the performance of our model is comparable to that of INQUERY and better than that of another language model for information retrieval. In particular, word pairs are shown to be useful in improving the retrieval performance.},
	booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
	pages = {316–321},
	numpages = {6},
	keywords = {good-turing estimate, statistical language modeling, curve-fitting functions, model combinations},
	location = {Kansas City, Missouri, USA},
	series = {CIKM '99}
}

@phdthesis{hiemstra_lm,
	author = {Hiemstra, Djoerd},
	title={{Using language models for information retrieval}},
	type = {PhD thesis},
	school = {Universiteit Twente},
	year = {2001}
}

@inproceedings{zhai_lm,
	author = {Zhai, ChengXiang and Lafferty, John},
	title = {{Two-Stage Language Models for Information Retrieval}},
	year = {2002},
	isbn = {1581135610},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/564376.564387},
	doi = {10.1145/564376.564387},
	abstract = {The optimal settings of retrieval parameters often depend on both the document collection and the query, and are usually found through empirical tuning. In this paper, we propose a family of two-stage language models for information retrieval that explicitly captures the different influences of the query and document collection on the optimal settings of retrieval parameters. As a special case, we present a two-stage smoothing method that allows us to estimate the smoothing parameters completely automatically. In the first stage, the document language model is smoothed using a Dirichlet prior with the collection language model as the reference model. In the second stage, the smoothed document language model is further interpolated with a query background language model. We propose a leave-one-out method for estimating the Dirichlet parameter of the first stage, and the use of document mixture models for estimating the interpolation parameter of the second stage. Evaluation on five different databases and four types of queries indicates that the two-stage smoothing method with the proposed parameter estimation methods consistently gives retrieval performance that is close to---or better than---the best results achieved using a single smoothing method and exhaustive parameter search on the test data.},
	booktitle = {Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
	pages = {49–56},
	numpages = {8},
	keywords = {Dirichlet prior, two-stage smoothing, mixture model, risk minimization, parameter estimation, interpolation, two-stage language models, leave-one-out},
	location = {Tampere, Finland},
	series = {SIGIR '02}
}

@article{psychology-reproduciblity,
	author = {{Open Science Collaboration}},
	title = {{Estimating the reproducibility of psychological science}},
	journal = {Science},
	volume = {349},
	number = {6251},
	pages = {aac4716},
	year = {2015},
	doi = {10.1126/science.aac4716},
	eprint = {https://www.science.org/doi/pdf/10.1126/science.aac4716},
	abstract = {One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts et al. describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study. Science, this issue 10.1126/science.aac4716 A large-scale assessment suggests that experimental reproducibility in psychology leaves a lot to be desired. Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.}
}

@inproceedings{Armstrong-dontaddup,
	author = {Armstrong, Timothy G. and Moffat, Alistair and Webber, William and Zobel, Justin},
	title = {{Improvements That Don't Add up: Ad-Hoc Retrieval Results since 1998}},
	year = {2009},
	isbn = {9781605585123},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/1645953.1646031},
	doi = {10.1145/1645953.1646031},
	abstract = {The existence and use of standard test collections in information retrieval experimentation allows results to be compared between research groups and over time. Such comparisons, however, are rarely made. Most researchers only report results from their own experiments, a practice that allows lack of overall improvement to go unnoticed. In this paper, we analyze results achieved on the TREC Ad-Hoc, Web, Terabyte, and Robust collections as reported in SIGIR (1998--2008) and CIKM (2004--2008). Dozens of individual published experiments report effectiveness improvements, and often claim statistical significance. However, there is little evidence of improvement in ad-hoc retrieval technology over the past decade. Baselines are generally weak, often being below the median original TREC system. And in only a handful of experiments is the score of the best TREC automatic run exceeded. Given this finding, we question the value of achieving even a statistically significant result over a weak baseline. We propose that the community adopt a practice of regular longitudinal comparison to ensure measurable progress, or at least prevent the lack of it from going unnoticed. We describe an online database of retrieval runs that facilitates such a practice.},
	booktitle = {Proceedings of the 18th ACM Conference on Information and Knowledge Management},
	pages = {601–610},
	numpages = {10},
	keywords = {survey, system measurement, retrieval experiment, evaluation},
	location = {Hong Kong, China},
	series = {CIKM '09}
}

@article{big-graphs,
	author = {Sakr, Sherif and Bonifati, Angela and Voigt, Hannes and Iosup, Alexandru and Ammar, Khaled and Angles, Renzo and Aref, Walid and Arenas, Marcelo and Besta, Maciej and Boncz, Peter A. and Daudjee, Khuzaima and Valle, Emanuele Della and Dumbrava, Stefania and Hartig, Olaf and Haslhofer, Bernhard and Hegeman, Tim and Hidders, Jan and Hose, Katja and Iamnitchi, Adriana and Kalavri, Vasiliki and Kapp, Hugo and Martens, Wim and \"{O}zsu, M. Tamer and Peukert, Eric and Plantikow, Stefan and Ragab, Mohamed and Ripeanu, Matei R. and Salihoglu, Semih and Schulz, Christian and Selmer, Petra and Sequeda, Juan F. and Shinavier, Joshua and Sz\'{a}rnyas, G\'{a}bor and Tommasini, Riccardo and Tumeo, Antonino and Uta, Alexandru and Varbanescu, Ana Lucia and Wu, Hsiang-Yun and Yakovets, Nikolay and Yan, Da and Yoneki, Eiko},
	title = {{The Future is Big Graphs: A Community View on Graph Processing Systems}},
	year = {2021},
	issue_date = {September 2021},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {64},
	number = {9},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/3434642},
	doi = {10.1145/3434642},
	abstract = {Ensuring the success of big graph processing for the next decade and beyond.},
	journal = {Commun. ACM},
	month = {aug},
	pages = {62–71},
	numpages = {10}
}

@inproceedings{angles2018property,
	title={{Property Graph Database Model.}},
	author={Angles, Renzo},
	booktitle={AMW},
	year={2018},
	booktitle={Proceedings of the 12th Alberto Mendelzon Workshop on Foundations of Data Management},
	series={AMW' 18},
	location={Cali, Colombia},
}

@inproceedings{clueweb22,
	author = {Overwijk, Arnold and Xiong, Chenyan and Callan, Jamie},
	title = {{ClueWeb22: 10 Billion Web Documents with Rich Information}},
	year = {2022},
	isbn = {9781450387323},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3477495.3536321},
	doi = {10.1145/3477495.3536321},
	abstract = {ClueWeb22, the newest iteration of the ClueWeb line of datasets, is the result of more than a year of collaboration between industry and academia. Its design is influenced by the research needs of the academic community and the real-world needs of large-scale industry systems. Compared with earlier ClueWeb datasets, the ClueWeb22 corpus is larger, more varied, and has higher-quality documents. Its core is raw HTML, but it includes clean text versions of documents to lower the barrier to entry. Several aspects of ClueWeb22 are available to the research community for the first time at this scale, for example, visual representations of rendered web pages, parsed structured information from the HTML document, and the alignment of document distributions (domains, languages, and topics) to commercial web search.This talk shares the design and construction of ClueWeb22, and discusses its new features. We believe this newer, larger, and richer ClueWeb corpus will enable and support a broad range of research in IR, NLP, and deep learning.},
	booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	pages = {3360–3362},
	numpages = {3},
	keywords = {web corpus, dataset, clueweb},
	location = {Madrid, Spain},
	series = {SIGIR '22}
}

@inproceedings{bast-etal-2023-fair,
	title = {{A Fair and In-Depth Evaluation of Existing End-to-End Entity Linking Systems}},
	author = "Bast, Hannah  and
	Hertel, Matthias  and
	Prange, Natalie",
	editor = "Bouamor, Houda  and
	Pino, Juan  and
	Bali, Kalika",
	booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
	month = dec,
	year = "2023",
	address = "Singapore",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.emnlp-main.411",
	doi = "10.18653/v1/2023.emnlp-main.411",
	pages = "6659--6672",
	abstract = "Existing evaluations of entity linking systems often say little about how the system is going to perform for a particular application. There are two fundamental reasons for this. One is that many evaluations only use aggregate measures (like precision, recall, and F1 score), without a detailed error analysis or a closer look at the results. The other is that all of the widely used benchmarks have strong biases and artifacts, in particular: a strong focus on named entities, an unclear or missing specification of what else counts as an entity mention, poor handling of ambiguities, and an over- or underrepresentation of certain kinds of entities. We provide a more meaningful and fair in-depth evaluation of a variety of existing end-to-end entity linkers. We characterize their strengths and weaknesses and also report on reproducibility aspects. The detailed results of our evaluation can be inspected under https://elevant.cs.uni-freiburg.de/emnlp2023. Our evaluation is based on several widely used benchmarks, which exhibit the problems mentioned above to various degrees, as well as on two new benchmarks, which address the problems mentioned above. The new benchmarks can be found under https://github.com/ad-freiburg/fair-entity-linking-benchmarks.",
}

@article{on-fuhrs-guideline,
	author = {Sakai, Tetsuya},
	title = {{On Fuhr's guideline for IR evaluation}},
	year = {2021},
	issue_date = {June 2020},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {54},
	number = {1},
	issn = {0163-5840},
	url = {https://doi.org/10.1145/3451964.3451976},
	doi = {10.1145/3451964.3451976},
	abstract = {In the December 2017 issue of SIGIR Forum, Fuhr presented ten "Thou Shalt Not"s (i.e., warnings against bad practices) for IR experimenters. While his article provides a lot of good materials for discussion, the objective of the present article is to argue that not all of his recommendations should be considered as absolute truths: researchers should be aware that there are other views; conference programme chairs and journal editors should be very careful when providing a guideline for evaluation practices.},
	journal = {SIGIR Forum},
	month = feb,
	articleno = {12},
	numpages = {8}
}

\chapter{IR using Relational Databases}
\label{ir-using-relational-databases}
\epigraph{The Analytical Engine has no pretensions whatever to originate anything. It can do whatever we know how to order it to perform}{Ada Lovelace - 1843}

\begin{Abstract}
	\begin{changemargin}{1cm}{1cm}
		Throughout the years, there have been many attempts to express information retrieval problems using relational databases. This chapter will highlight one of the latter attempts that revived the idea of expressing bag-of-words ranking functions using SQL. A prototype system that uses these expressions is presented, dubbed OldDog, after the work by \citeauthor{OldDog} (\emph{Old Dogs Are Great at New Tricks: Column Stores for IR Prototyping}). This system can be used for rapid IR prototyping and is especially helpful in the context of reproducible information retrieval research. Also, when researchers report that they used BM25, it is not always clear which variant they mean. Many tweaks to \citeauthor{bm25-robertson}'s original formulation have been proposed. Does this ambiguity ``matter''? We attempt to answer this question with a large-scale reproducibility study of BM25, considering eight variants implemented in the OldDog system. Experiments on three newswire collections show no significant effectiveness differences between them, not even for Lucene's (often maligned) approximation of document length.
	\end{changemargin}
\end{Abstract}

\section{Introduction}
Where information retrieval researchers commonly use inverted indexes as data structures, there is also a rich history of researchers using relational databases to represent the data in information retrieval systems. Different approaches in the literature present varying degrees of succes. Given this context, we arrive at the first research question: 
\begin{itemize}  
	\item \emph{Research Question 1: What are the benefits of using relational databases for information retrieval?} 
\end{itemize}
In order to answer this question, first, we review the history of using database systems for IR. Then, one of the latter attempts of using a relational database for information retrieval will be highlighted. Using this work, a prototype system is built, dubbed ``OldDog''. This system will be used in a reproduction experiment, which compares several variants of BM25 with each other. This reproduction study confirms and rebuts previous findings from the literature and verifies that relational database systems are suited for running IR experiments. 

\section{Related work}

\subsection{Boolean retrieval}
Perhaps the earliest work on using relational databases for information retrieval is the work by~\citet{SchekPistor}. In their work, the authors recognize that the relational data model is widely accepted as an interface to query structured data. However, it is inconvenient to use unstructured data like text. They proposed extending the relational model by allowing Non-First Normal-Form ($\text{NF}^2$) relations. This extension allows for text queries to be more easily expressed. The systems that can be built in this language are still Boolean retrieval systems (as described in \cref{sec:boolean}). At the time, that worked well, but scoring was not a feature considered.  
Similarly, \citet{macleod} compared the inverted index approach to using the relational model. \Citeauthor{macleod} showed how queries of the IBM STAIRS system could be expressed using the relational model. These were, however, still Boolean queries, so scoring using uncertainty was not considered. 

\subsection{Probabilistic Relational Algebra}
\Citet{fuhr1996probabilistic} recognized that where databases contain structured/formatted data, IR systems deal with unformatted data requiring uncertain inference. They propose to express this uncertainty using a probabilistic relational algebra (PRA)~\citep{fuhr-pra}. 
PRA can be considered an extension of standard relational algebra. 
The basic idea behind PRA is that tuples are assigned weights; the weight represents the probability that the tuple belongs to the relation. These probabilities give two advantages. Uncertainty of information can be expressed explicitly, and tuples representing answers to queries can be ordered by the weights representing this uncertainty. Tuples with high probabilities are ranked higher than those with lower ones. Although these extensions give advantages over Boolean retrieval, how to assign these probabilities to, for example, a document-term pair remains a question.

\subsection{IR on top of a database cluster}
% T. Grabs, K. Bhoem, and H.-J. Schek. PowerDB-IR: scalable information retrieval and storage with a cluster of databases. Knowledge and Information Systems, 6(4):465–505, 2004.
\Citet{PowerDB-IR} have proposed PowerDB-IR, developed to run IR applications on a scalable infrastructure. It should also be able to update the data quickly while retrieving up-to-date results. \Citeauthor{PowerDB-IR} achieve this by assigning every document to a category, e.g., sports or news, in their experiment. A dedicated node is created for every category, containing documents, inverted lists, and statistics tables. 
The system supports both single-category and multi-category searches. For a ``single query'' search, the following ranking score value is calculated:
\begin{equation}
	\text{RSV}(d,q) = \sum_{t\in q} \mathit{tf}(t, d) \cdot \mathit{idf}(t)^2 \cdot \mathit{tf}(t, q)
\end{equation}
Here $\mathit{tf}(t, d)$ is the term frequency of term $t$ in document $d$, $\mathit{idf}(t)$ is the inverted document frequency of term $t$ (which is squared in this formula), and $\mathit{tf}(t, q)$ is the term frequency of term $t$ in the query text. Calculating this is straightforward: all statistics necessary are stored on the node decided to that category. However, when one wants to search on multiple (or all) categories, subscores need to be calculated for all relevant nodes before they can be aggregated to a final score. This approach is quite costly, as variances between executions times on the nodes are high, but this work may have proposed the first real IR in SQL approach. 

\subsection{Integrating DB + IR}
% S. Chaudhuri, R. Ramakrishnan, and G. Weikum. Integrating DB and IR technologies: What is the sound of one hand clapping? CIDR, 2005.
\Citet{Chaudhuri2005IntegratingDA} also identified the need for systems that integrate database and IR functionalities. In their view, database systems need to be more flexible for scoring and ranking, while IR systems cannot adequately handle structured data and metadata. \Citeauthor{Chaudhuri2005IntegratingDA} put together a list of seven requirements that a DB + IR system should be able to support, of which they identified the following three requirements as the most important:
\begin{enumerate}
	\item \emph{Flexible scoring and ranking.}
	It should be possible to customize the ranking function for different applications; a news search system probably needs different ranking functions and settings than a web search system. 
	\item \emph{Optimizability.}
	Following standard database approaches, queries in a DB+IR system should have a query optimizer that considers the workload and the data characteristics. For example, when only one relevant result is sufficient, the system should be able to abort when a relevant document is found. 
	\item \emph{Metadata and ontologies.}
	Other than metadata that describes data sources, metadata that is used for understanding information demands might be needed. This metadata could be, for example, an ontology or a lexicon used for more effective ranking strategies.\footnote{Latent representations generated by large language models would have been a great example of this kind of metadata had their paper been written today.}
\end{enumerate}
To build a system that can support these requirements, the authors identified four alternatives for designing a DB+IR system:
\begin{enumerate}
	\item \emph{On-top-of-SQL.} The IR functionalities are built on top of a SQL engine. The disadvantage of this approach is that it is challenging to customize efficient access for both IR and DB functionalities. 
	\item \emph{Middleware.} In this approach, SQL and IR engines run simultaneously. The two disadvantages of using this approach are that the API needs to talk to two systems, which can have very different design philosophies, and the data needs to be shared between systems, incurring a large overhead and making it harder to combine both functionalities. 
	\item \emph{IR-via-ADTs.} The third approach is building an IR system using abstract data types (ADT). The authors argue that this approach makes the system more customizable than the previous approaches. However, the authors also note that query optimization in the presence of user defined functions is complicated. Also, when programmers need to work with such a system, it has the full complexity of SQL plus the complexity of working with ADTs. 
	\item \emph{RISC.} The final approach is what the authors prefer; IR functionalities build on top of a relational \textit{storage} engine, as described in an earlier work by them~\citep{risc}. The DB+IR systems should then be built on top of this engine. The storage-level core should be build ``Reduced Instruction Set Computing'' style (RISC). 
\end{enumerate}  
Although the approaches described in this work are interesting, they do not provide prototypes to compare them. (The goal of this paper was to present a theoretical framework for tackling this problem.) Even though it is not the preferred option of \citeauthor{Chaudhuri2005IntegratingDA}, this PhD thesis research has focused on the \emph{On-top-of-SQL} approach, assuming that recent developments in database system engineering have overcome the shortcomings with respect to ranking efficiency.

\subsection{Handwritten plans and Array Databases}
% S. H´eman, M. Zukowski, A. de Vries, and P. A. Boncz. MonetDB/X100 at the 2006 TREC terabyte track. TREC, 2006.
\Citet{handwritten} participated in the TREC TeraByte track using the relational engine MonetDB/X100 \citep{monetdb/x100}. They were able to express ranking functions efficiently and effectively in this system. In their participation, they used BM25 as a scoring function. In order to reduce the amount of computation necessary for every document-term pair, the BM25 score was precalculated. 
A shortcoming in this approach is that the query plans were not generated from SQL, but handwritten (essentially the RISC approach of above). Having to handwrite queries makes the system challenging to use for IR researchers. Also, because all BM25 scores were precalculated (albeit with some compression), more storage was needed than when only the term frequencies would be saved.

% R. Cornacchia, S. H´eman, M. Zukowski, A. de Vries, and P. Boncz. Flexible and efficient IR using array databases. VLDB, 2008.
The same research group~\citep{array-db} also ran experiments on the TREC TeraByte track using the array database SRAM (Sparse Relational Array Mapping). SRAM automatically translates BM25 queries to run them on a relational engine (specifically X100). However, SRAM is quite an esoteric query language, only used by the researchers themselves, and no publicly available (prototype) system offers support for this approach. 

\subsection{Retrieval models only using SQL}
In more recent work, \citet{OldDog} showed that the commonly used BM25 ranking function could be expressed easily using SQL. This is done similarly to \citet{PowerDB-IR}. In this work, the MonetDB~\citep{monet} and Vectorwise~\citep{vectorwise} systems were used, making the runtime much faster than~\citeauthor{PowerDB-IR}'s original results. \Citeauthor{OldDog} specifically focused on the retrieval efficiency of systems and compared the efficiency of inverted indexes with systems built on top of relational engines. They argued that instead of using a custom build IR system using an inverted index, researchers could store their data representations in a column-oriented relational database and formulate the ranking functions using SQL. They show that their implementation of BM25 in SQL is on par in efficiency and effectiveness compared to systems that use an inverted index.\footnote{This is especially the case for the Vectorwise system.}  

There was an interesting observation in the paper to highlight: All the systems evaluated in this paper implement a ranking function they refer to as BM25. However, there was a substantial difference between the effectiveness scores produced by these systems, as shown in \cref{olddog_results}. The only two systems that achieved the same effectiveness score were the two database systems (MonetDB and Vectorwise).\footnote{Although the same research group developed these two systems, they were completely separate projects.}

\begin{table}
	\centering
	\caption{Results presented by~\citet{OldDog}. \texttt{MAP} and \texttt{P@5} on the ClueWeb12 collection are reported for five different systems that each claim to rank their documents using BM25. The table shows that only the two database systems (MonetDB and Vectorwise) achieve the same effectiveness score.}
	\label{olddog_results}
	\begin{tabular}{c c c}
		\toprule
		System &  \texttt{MAP} & \texttt{P@5} \\
		\midrule
		Indri~\citep{indri} & 0.246 & 0.304 \\
		MonetDB~\citep{monet} & 0.225 & 0.276 \\
		Vectorwise~\citep{vectorwise} & 0.225 & 0.276 \\
		Lucene~\citep{lucene} & 0.216 & 0.265 \\
		Terrier~\citep{terrier} & 0.215 & 0.272 \\
		\bottomrule
	\end{tabular}
\end{table}

These results were surprising, as the authors took specific care to keep document preprocessing identical for all systems. Still, the observed difference in \texttt{MAP} of 3\% absolute was the largest deviation in the scores reported. 

\subsection{Reproducibility}
The paper by~\citet{OldDog} is not the only one that reports differences in effectiveness scores for BM25. In the SIGIR 2015 Workshop on Reproducibility, Inexplicability, and Generalizability of Results (RIGOR)~\citep{RIGOR} and the Open-Source IR Replicability Challenge (OSIRRC) workshop~\citep{OSIRRC} similar results can be observed. See Tables  \ref{rigor_results} and \ref{osirrc_results}, respectively. % no cref here because plural.  

\begin{table}
	\centering
	\caption{Results from the RIGOR workshop~\citep{RIGOR}. \texttt{MAP@1000} on the .GOV2 collection is reported for four different systems that run BM25. The table shows that all four implementations report a different effectiveness score.}
	\label{rigor_results}
	\begin{tabular}{c c}
		\toprule
		System &  \texttt{MAP@1000} \\
		\midrule
		ATIRE~\citep{ATIRE} & 0.290 \\
		Lucene~\citep{lucene} & 0.303 \\
		MG4J~\citep{MG4J} & 0.299 \\
		Terrier~\citep{terrier} & 0.270 \\
		\bottomrule
	\end{tabular}
\end{table}

\begin{table}
	\centering
	\caption{Results from the OSIRRC workshop~\citep{OSIRRC}. \texttt{AP}, \texttt{P@30}, and \texttt{NDCG@20} on the robust04 collection are reported for seven different systems that run BM25. As shown in the table, all implementations report (again) a different effectiveness score.}
	\label{osirrc_results}
	\begin{tabular}{c c c c}
		\toprule
		System & \texttt{AP} & \texttt{P@30} & \texttt{NDCG@20} \\
		\midrule
		Anserini~\citep{anserini-docker} & 0.253 & 0.310 & 0.424 \\
		ATIRE~\citep{ATIRE} & 0.218 & 0.320 & 0.421 \\
		ielab~\citep{ielab} & 0.183 & 0.261 & 0.348 \\
		Indri~\citep{indri-docker} & 0.239 & 0.300 & 0.404 \\
		OldDog~\citep{olddog-docker} & 0.243 & 0.299 & 0.400 \\
		Pisa~\citep{pisa} & 0.253 & 0.312 & 0.422 \\
		Terrier~\citep{terrier-docker} & 0.236 & 0.298 & 0.405 \\
		\bottomrule
	\end{tabular}
\end{table}

It is unclear why the results between these systems differ this much; many explanations are possible. Examples include; differences in preprocessing\footnote{But this cannot explain the differences reported in~\citeauthor{OldDog}, as they ensured preprocessing was the same for all systems.}, different hyperparameter settings, differences in the definition of the inverse document frequency ($idf$) used, or erroneous implementation of the ranking function. Using, for example, non-optimized hyperparameter settings can lead to considerable gaps in differences between effectiveness scores. \citet{weak-baselines} showed that in many cases, new ranking methods had been proposed that compared the results of a newly proposed method to a non-fine-tuned version of BM25, making the results look better than they are. The choices for the BM25 hyperparameters are often left out of papers where it is the baseline compared against. As BM25 is often used as a baseline, it is crucial to understand why these differences exist and how they arise.

\section{Prototype OldDog}
As shown in~\cref{osirrc_results}, one of the workshop's submissions was the prototype system developed for this PhD thesis research~\citep{olddog-docker}. This prototype is a software project to replicate and extend the database approach to information retrieval presented in~\citet{OldDog}. The prototype was based on their work, so we dubbed it \emph{OldDog}. OldDog uses column store database MonetDB~\citep{monet} for query processing. 
\Citeauthor{OldDog} produced the database tables to represent the data typically found in an inverted index, using a custom program run on Hadoop.
Instead, we created a Lucene index using the Anserini tool suite developed by~\citet{anserini}. Anserini takes care of standard document preprocessing. From Anserini's Lucene index, we extracted the data necessary to fill the tables. 
Three tables are constructed. One table contains the data that represents the documents, another one that represents the terms, and one that contains all data that relate terms to documents. To illustrate, the results for a document named ``doc1'' with the text ``I put on my coat and green hat'' shown in \cref{olddog-tables}. 

\begin{table}
	\centering
	\caption{Example of tables representing the data in the OldDog system, the dict tables contains all term specific data, the terms table represents all the postings, and the docs table contains all document specific data.}
	\begin{subtable}[t]{0.31\linewidth}
		\centering
		\caption{\textbf{dict}}
		\(\begin{array}{ccc}
			\toprule
			\text{termid} & \text{term} & \text{df}  \\  
			\midrule
			1 & \text{put} 	 	& 1 \\ 
			2 & \text{coat}  	& 1 \\ 
			3 & \text{green}	& 1 \\ 
			4 & \text{hat} 	 	& 1 \\
			\bottomrule
		\end{array}\)
	\end{subtable}
	\begin{subtable}[t]{0.31\linewidth}
		\centering
		\caption{\textbf{docs}}
		\(\begin{array}{ccc}
			\toprule
			\text{docid} & \text{name} & \text{length}  \\  
			\midrule
			1 & \text{doc1} & 8 \\ 
			\bottomrule
		\end{array}\)
	\end{subtable}
	\begin{subtable}[t]{0.31\linewidth}
		\centering
		\caption{\textbf{terms}}
		\(\begin{array}{ccc}
			\toprule
			\text{termid} & \text{docid} & \text{tf}  \\  
			\midrule
			1 & 1 & 1 \\ 
			2 & 1 & 1 \\ 
			3 & 1 & 1 \\ 
			4 & 1 & 1 \\
			\bottomrule
		\end{array}\)
	\end{subtable}
	\label{olddog-tables}
\end{table}

Using these tables, we can easily express bag-of-words ranking functions in SQL queries. The default ranking function implemented in our implementation of OldDog is the version of BM25 that had been proposed by~\citet{bm25-robertson}.

\subsection{Docker}
For the submission to the OSIRRC workshop~\citep{OSIRRC}, we created a docker image of OldDog\footnote{\url{https://hub.docker.com/r/osirrc2019/olddog}, last accessed - November 14th 2024}~\citep{olddog-docker}. \Citeauthor{OldDog} implemented a conjunctive variant of BM25 (all query terms have to be present in a document in order for a document to be considered relevant). When creating the submission for the workshop, we noticed that the effectiveness scores were substantially lower than those of other submissions. The retrieval effectiveness degraded more than we expected, considering the results in previous work. When removing the conjunctive constraint, the effectiveness of the system increased. So our prototype supports both conjunctive and disjunctive versions of BM25. Our entry in~\cref{osirrc_results} presents the effectiveness scores of the disjunctive variant. The number of relevant documents per topic for this collection was likely low; decreasing the effectiveness of the system when imposing a conjunctive constraint.

\subsection{Ease-of-Use}
Having implemented BM25 in a database system enabled us to carry out some experiments quite easily that are more complicated when using an inverted index. Filtering out the terms with a large document frequency is easy, as all document frequencies are stored in one table. We updated the table removing the terms with large document frequency in only two lines of SQL, as shown in~\cref{fig:remove-high-df}. This approach could be an automatic way to remove stopwords from a collection. This filter was too strict to improve retrieval effectiveness but can easily be fine-tuned. For example, we could join against a table storing a traditional stopword list. 

\begin{figure}
	\begin{minted}[linenos, breaklines, breakafter=-, escapeinside=||]{sql}
CREATE table dict AS SELECT * FROM odict WHERE df <= (
	SELECT 0.1 * COUNT(*) FROM docs
);		
	\end{minted}
	\caption{This code updates the \texttt{docs} table such that all terms with a document frequency greater than a tenth of the collection size are removed.}
	\label{fig:remove-high-df}
\end{figure}

\section{Variants of BM25}
\label{ir_db_variants}
Having ``OldDog'' set up, we can run retrieval experiments easily. As mentioned in the previous section, it is still unclear why the differences between implementations of BM25 of different systems were this big. Also, many different variants of BM25 have been proposed in the literature, that claim to be more effective than the original formulation. A study by \citet{trotman-bm25} compared several variants and found that improvements presented in the literature cannot be reproduced. 
As we now have a system in which the BM25 formula is written directly in SQL, we can easily swap this version of BM25 with its proposed improvements. By using OldDog, we can ensure the data representation is the same when we compare these variants; the results will only reflect what the effects are of applying a different variant of BM25. This way, we can easily validate \citeauthor{trotman-bm25}'s findings.

\subsubsection{\Citet{bm25-robertson}} 
The original formulation of BM25 consists of two parts. The first part is derived from the binary independence relevance model~\citep{bm25-beyond}, which results in an approximation of the classical inverse document frequency ($\mathit{idf}$) for a query term $t$:

\begin{equation} 
	w_t^{\text{IDF}} = \log\left(\frac{N-\mathit{df}_t+0.5}{\mathit{df}_t+0.5}\right)
\end{equation}
where $N$ is the collection size, and $df_t$ are the number of collection documents containing query term $t$.  

There is, however, a negative consequence of using this formula for weighting term importance. Assume a collection with $10,000$ documents; then, it is possible to plot the $\mathit{idf}$ for each term as shown in~\cref{idf}. The figure shows that the $\mathit{idf}$ score becomes negative when $\mathit{df}_t > \frac{N}{2}$. This happens for terms that appear in more than half of all documents, e.g.: ``the'' or ``a''. Many systems do not consider these terms when searching by keeping a list of common words that can be ignored (stop words). However, when these words are considered,  a negative $\mathit{idf}$ would decrease the relevance scores of documents with the query term in the document -- variations of BM25 have been proposed to deal with this anomaly, that are discussed in the following sections. 

\begin{figure}
	\begin{tikzpicture}
		\begin{axis}[
			axis lines = left,
			xlabel = \(\mathit{df}_t\),
			ylabel = {\(\mathit{idf}\)},
			]
			%Below the red parabola is defined
			\addplot [
			domain=0:10000, 
			samples=1000, 
			color=red,
			]
			{ln((10000-x+0.5)/(x+0.5))};
			\addlegendentry{\(ln(\frac{10000-df_t+0.5}{df_t+0.5})\)}
		\end{axis}
	\end{tikzpicture}
	\caption{Inverse document frequency as used by \citet{bm25-robertson}}
	\label{idf}
\end{figure} 

The second part of BM25 can be considered as a term frequency weighting $\mathit{tf}$. The two parts are multiplied to get something like the traditional term frequency-inverse document frequency weighting $\mathit{tf} \times w_i^{\text{IDF}}$.
However, the $\mathit{tf}$ in BM25 is not just a linear factor; every additional term occurrence increases the ranking score value less than the previous one. For example, a term being present twice in a document versus once provides more information than a term being present ten times versus nine. To achieve this effect of ``diminishing returns'' of additional occurrences, the following formula defines the \textit{tf} component of the ranking:

\begin{align}
	\frac{\mathit{tf}}{k+\mathit{tf}} & \text{ where } k>0
\end{align}

This approach ensures that the term frequency does not increase linearly. In the final formulation of BM25, $k$ is written as $k_1$, because earlier versions of this ranking formula also had $k_2$ and $k_3$ parameters.

Lastly, a second component is added to correct for variation in document length. It is, however, unclear how one should deal with documents being longer than others; the document's author can be verbose, in which case additional term occurrences do not provide extra information. On the other hand, a document can be lengthy because more relevant information is provided, and the document is more relevant than its shorter counterpart. For these reasons, the following soft length normalization is introduced:

\begin{align}
	\left(1-b\right) + b \times \left(\frac{L_d}{L_{\mathit{avg}}}\right) & \text{ with } 0 <= b <= 1
\end{align}

The value of $b$ controls the degree of length normalization. Combining these parts, including the correction for term frequency and length normalization, we get BM25 as initially proposed by \citet{bm25-robertson}:

\begin{equation}
	\label{bm25-robertson}
	\sum_{t\in q} \log\left(\frac{N-\mathit{df}_t+0.5}{df_t+0.5}\right)\cdot\frac{\mathit{tf}_{\mathit{td}}}{k_1\cdot\left(1-b+b\cdot\left(\frac{L_d}{L_{\mathit{avg}}}\right)\right) + \mathit{tf}_{\mathit{td}}}
\end{equation}

\subsubsection{Lucene (default)}
Lucene is a widely used open source search engine software library\footnote{\url{https://lucene.apache.org/}}. 
The variant of BM25 implemented in Lucene (as of version 8) introduces two main differences. As mentioned, the $\mathit{idf}$ component of \citet{bm25-robertson} is negative when $\mathit{df}_t > \frac{N}{2}$.
To avoid negative values in all possible cases, Lucene adds a constant value of one before calculating the $log$ value. 
Second, the document length used in the scoring function is compressed (in a lossy manner) to a one-byte value, denoted $L_{d\text{lossy}}$. With only 256 distinct document lengths, Lucene pre-computes for each possible length the value of:

\begin{equation}
	k_1 \cdot \left(1-b+b\cdot\left(\frac{L_{d\text{lossy}}}{L_{\mathit{avg}}}\right)\right)
\end{equation}

This results in fewer computations at query time. \Cref{lucene-default} describes BM25 as implemented in Lucene:

\begin{equation}
	\label{lucene-default}
	\sum_{t\in q}\log\left(1 + \frac{N-\mathit{df}_t+0.5}{\mathit{df}_t+0.5}\right)\cdot\frac{\mathit{tf}_{\mathit{td}}}{k_1\cdot\left(1-b+b\cdot\left(\frac{L_{d \text{lossy}}}{L_{\mathit{avg}}}\right)\right)+\mathit{tf}_{\mathit{td}}}
\end{equation}

\subsubsection{Lucene (accurate)}

\Cref{lucene-accurate} represents our attempt to measure the impact of Lucene’s lossy document length encoding. We implemented a variant that uses exact document lengths but is otherwise identical to the Lucene default.

\begin{equation}
	\label{lucene-accurate}
	\sum_{t\in q}\log\left(1 + \frac{N-\mathit{df}_t+0.5}{\mathit{df}_t+0.5}\right)\cdot\frac{\mathit{tf}_{\mathit{td}}}{k_1\cdot \left(1-b+b\cdot\left(\frac{L_d}{L_{\mathit{avg}}}\right)\right)+\mathit{tf}_{\mathit{td}}}
\end{equation}

\subsubsection{ATIRE~\citep{ATIRE}}
\Cref{atire-variant} shows BM25 as implemented by ATIRE; it implements the $\mathit{idf}$ component of BM25 as $\log(N/\mathit{df}_{t})$, which also avoids negative values. The TF component is multiplied by $k_1+1$ to make it look more like the classic RSJ weight~\citep{RSJ}; this does not affect the resulting ranked list, as all scores are scaled linearly with this factor.

\begin{equation}
	\label{atire-variant}
	\sum_{t\in q}\log\left(\frac{N}{\mathit{df}_t}\right)\cdot\frac{\left(k_1 + 1\right)\cdot \mathit{tf}_{\mathit{td}}}{k_1\cdot\left(1-b+b\cdot\left(\frac{L_{d}}{L_{\mathit{avg}}}\right)\right)+\mathit{tf}_{\mathit{td}}}
\end{equation}

\subsubsection{BM25L~\citep{bm25l}}
BM25L builds on the observation that BM25 penalizes longer documents too much when compared to shorter ones. The $\mathit{idf}$ component differs to avoid negative values. The TF component is reformulated as follows:
\begin{equation}
	\frac{\left(k_1+1\right)\cdot c_{\mathit{td}}}{k_1+c_{\mathit{td}}}  
\end{equation}
with 

\begin{equation}
	c_{\mathit{td}} = \frac{\mathit{tf}_{\mathit{td}}}{1 - b + b \cdot \left(\frac{L_d}{L_{\mathit{avg}}}\right)}  
\end{equation}
The $c_{td}$ component is further modified by adding a constant $\delta$, boosting the score for longer documents. The authors report using $\delta = 0.5$ for the highest effectiveness. \Cref{bm25l} presents the final formulation of BM25L:

\begin{equation}
	\label{bm25l}
	\sum_{t\in q} \log\left(\frac{N+1}{\mathit{df}_t + 0.5}\right)\cdot\frac{(k_1 + 1)\cdot(c_{\mathit{td}} + \delta)}{k_1 + (c_{\mathit{td}} + \delta)}
\end{equation}

\subsubsection{BM25+~\citep{bm25+}}
BM25+, as shown in \cref{bm25+}, encodes a general approach for dealing with the issue that ranking functions unfairly prefer shorter documents over longer ones. \Citeauthor{bm25+} propose adding a lower-bound bonus when a term appears at least once in a document. The difference with BM25L is a constant $\delta$ to the TF component. The $\mathit{idf}$ component is again changed to a variant that disallows negative values.

\begin{equation}
	\label{bm25+}
	\sum_{t\in q} \log\left(\frac{N+1}{\mathit{df}_t}\right)\cdot\left(\frac{\left(k_1 + 1\right)\cdot \mathit{tf}_{\mathit{td}}}{k_1\cdot\left(\left(1-b\right)+b\cdot\left(\frac{L_d}{L_{\mathit{avg}}}\right)\right)+\mathit{tf}_{\mathit{td}}}+\delta\right)
\end{equation}

\subsubsection{BM25-adpt~\citep{bm25-adpt}}
BM25-adpt is an approach that varies $k_1$ per term (i.e., uses term specific $k_1$ values). In the original formulation of BM25, $k_1$ can be considered a hyperparameter that regulates the increase of score for additional occurrences of a term; $k_1$ ensures that every additional occurrence gets discounted as it provides less information than its previous. However, \citeauthor{bm25-adpt} argued that this does not necessarily have to be the case. If there are many fewer documents with $t+1$ occurrences versus $t$, it should provide more information than when the number of documents is almost the same. In order to find the optimal term-specific $k_1$ value, the authors want to maximize the information gain for that particular query term. 
First, they identify the probability of selecting a document randomly from the collection that contains the term $q$ at least once as:

\begin{equation}
	p(1|0,q) = \frac{\mathit{df}_t+0.5}{N+1}
\end{equation}

The probability of a term occurring one more time is defined as:

\begin{equation}
	p(t+1|t,q) = \frac{\mathit{df}_{t+1}+0.5}{\mathit{df}_t+1}
\end{equation}

In both these formulas, constants $1$ and $0.5$ are added for smoothing to avoid zero probabilities. Then the information gain from $t$ to $t+1$ occurrences is computed by subtracting the initial probability: 

\begin{equation}
	G^t_q = \log_2\left(\frac{\mathit{df}_{t+1} + 0.5}{\mathit{df}_t+1}\right) - \log_2 \left(\frac{\mathit{df}_{t} + 0.5}{N+1}\right)
\end{equation}

Here $\mathit{df}_t$ is not defined as a standard document frequency but based on the length normalized term frequency:

\begin{equation}
	df_t = 
	\begin{cases}
		|D_{t|c_{td}\geq t-0.5}| & t > 1\\ 
		df(q) & t = 1\\
		N & t = 0
	\end{cases}
\end{equation}

In this case $\mathit{df}(q)$ is the ``normal'' document frequency, and $c_{\mathit{td}}$ is the same as in BM25L (using the pivoted method for length normalization introduced by~\cite{ctd}):

\begin{equation}
	c_{\mathit{td}} = \frac{\mathit{tf}_{\mathit{td}}}{1-b+b\cdot\left(\frac{L_d}{L_{\mathit{avg}}}\right)}
\end{equation}

This implies that for $t = 0$ that $\mathit{df}_t$ is equal to the number of documents in the collection, and for $t = 1$ it is equal to the ``normal'' document frequency. Otherwise, it will be the number of documents with at least $t$ occurrences of the term (rounded up) using the pivoted method $c_{\mathit{td}}$. 

Next, the information gain is calculated for $t \in \{0,\cdots,T\}$, until $G^t_q > G^{t+1}_q$. This threshold is chosen as a heuristic: When $t$ becomes large, the estimated information gain can be very noisy. So $T$ is chosen as the smallest value that breaks the worst burstiness rule~\citep{burstiness_rule} (the information gain starts decreasing). The optimal value for $k_1$ is then determined by finding the value for $k_1$ that minimizes the following equation:
\begin{equation}
	k'_1 = \argmin_{k_1} \sum_{t=0}^{T}\left(\frac{G^t_q}{G^1_q} - \frac{(k_1+1)\cdot t}{k_1+t}\right)^2
\end{equation}

Essentially, this gives a value for $k_1$ that maximizes information gain for that specific term; $k_1$ and $G^1_q$ are then plugged into the BM25-adpt formula: 

\begin{equation}
	\label{bm25-adpt}
	\sum_{t\in q}G_q^1\cdot\frac{\left(k'_1+1\right)\cdot \mathit{tf}_{\mathit{td}}}{k'_1\cdot\left(\left(1-b\right)+b\cdot\left(\frac{L_d}{L_{\mathit{avg}}}\right)\right)+\mathit{tf}_{\mathit{td}}}
\end{equation}

We found that the optimal value of $k_1$ is not defined for about 90\% of the terms. A unique optimal value for $k_1$ only exists when $t > 1$ while calculating $G^t_q$. For many terms, especially those with a low $\mathit{df}$, $G^t_q > G^{t+1}_q$ occurs before $t > 1$. In these cases, picking different values for $k_1$ has virtually no effect on retrieval effectiveness. For undefined values, we set $k_1$ to $0.001$, the same as \citet{trotman-bm25}.

\subsubsection{TF $l\circ\delta\circ p\times$IDF~\citep{tf-ldp-idf}}
TF$l\circ\delta\circ p\times$IDF, as shown in equation \ref{tf-ldp-idf}, models the non-linear gain of a term occurring multiple times in a document as:
\begin{equation}
	1+\log\left(1+\log\left(\mathit{tf}_{\mathit{td}}\right)\right) 
\end{equation}

To ensure terms occurring at least once in a document get boosted, the approach adds a fixed component $\delta$, following BM25+. These parts are combined into the TF component using the pivoted method for length normalization~\citep{ctd}:

\begin{equation}
	c_{\mathit{td}} = \frac{\mathit{tf}_{\mathit{td}}}{1-b+b\cdot\left(\frac{L_d}{L_{\mathit{avg}}}\right)}
\end{equation}

The same IDF component used as in BM25+, which gives us TF$l\circ\delta\circ p\times$IDF: 

\begin{equation}
	\label{tf-ldp-idf}
	\sum_{t\in q}\log\left(\frac{N+1}{\mathit{df}_t}\right)\cdot\left(1+\log\left(1+\log\left(c_{\mathit{td}}+\delta\right)\right)\right)
\end{equation}

\section{Experiments}
This section presents an empirical evaluation of the impact of the different choices of BM25 as described in~\cref{ir_db_variants}.
Our experiments were conducted using Anserini (v0.6.0) on Java 11 to create an initial index, and subsequently using relational databases for rapid prototyping, using ``OldDog''~\citep{olddog-docker} after~\citet{OldDog}; following that work, we use MonetDB as the column-oriented analytic database. Evaluations with Lucene (default) and Lucene (accurate) were performed directly in Anserini; the latter was based on previously-released code that we updated and incorporated into Anserini.\footnote{\url{http://searchivarius.org/blog/accurate-bm25-similarity-lucene}, last accessed - November 14th 2024} The inverted index was exported from Lucene to OldDog, ensuring that all experiments share the same document processing pipeline (e.g., tokenization, stemming, stopword removal). While exporting the inverted index, we precalculate all $k_1$ values for BM25-adpt as suggested by \citet{bm25-adpt}. As an additional verification step, we implemented both Lucene (default) and Lucene (accurate) in OldDog and compared the results to the output from Anserini. We can confirm that the results are the same, setting aside unavoidable differences related to floating point precision. All BM25 variants are then implemented in OldDog as minor variations on the original SQL query provided in \citet{OldDog}. The term-specific parameter optimization for the \emph{adpt} variant was already calculated during the index extraction stage, allowing us to upload the optimal $(t, k)$ pairs and directly use the term-specific $k$ values in the SQL query. The advantage of our experimental methodology is that we did not need to implement a single new ranking function from scratch.

The experiments use three TREC newswire test collections: TREC Disks 4 and 5, excluding Congressional Record, with topics and relevance judgments from the TREC 2004 Robust Track (Robust04); the New York Times Annotated Corpus, with topics and relevance judgments from the TREC 2017 Common Core Track (Core17); the TREC Washington Post Corpus, with topics and relevance judgments from the TREC 2018 Common Core Track (Core18). Following standard experimental practice, we assess ranked list output in terms of average precision (\texttt{AP}) and precision at rank 30 (\texttt{P@30}). The parameters shared by all models are set to $k_1$ = 0.9 and $b$ = 0.4, Anserini’s defaults. The parameter $\delta$ is set to the value reported as best in the corresponding source publication. 

All experiments were run on a Linux desktop (Fedora 30, Kernel 5.2.18, SELinux enabled) with four cores (Intel Xeon CPU E3-1226 v3 @ 3.30 GHz) and 16 GB of main memory; the MonetDB 11.33.11 server was compiled from source using the \texttt{---enable-optimize} flag.

\section{Results}
\Cref{bm25_variant_results} shows the effectiveness scores of the different BM25 variants.  
\begin{table}
	\centering
	\caption{Effectiveness scores different BM25 variants. All were implemented as SQL queries, so the underlying data representations are the same.}
	\label{bm25_variant_results}
	\begin{tabular}{l c c c c c c}
		\toprule
		&\multicolumn{2}{c}{Robust04}&\multicolumn{2}{c}{Core17}&\multicolumn{2}{c}{Core18}\\
		&\texttt{AP}&\texttt{P@30}&\texttt{AP}&\texttt{P@30}&\texttt{AP}&\texttt{P@30}\\
		\midrule
		{\small \citeauthor{bm25-robertson}} & .2526 & .3086 & .2094 & .4327 & .2465 & \textbf{.3647} \\ 
		{\small Lucene (default)} & .2531 & .3102 & .2087 & .4293 & .2495 & .3567 \\ 
		{\small Lucene (accurate)} & .2533 & .3104 & .2094 & .4327 & .2495 & .3593 \\ 
		{\small ATIRE} & .2533 & .3104 & .2094 & .4327 & .2495 & .3593 \\ 
		{\small BM25L} & .2542 & .3092 & .1975 & .4253 & \textbf{.2501} & .3607 \\ 
		{\small BM25+} & .2526 & .3071 & .1931 & .4260 & .2447 & .3513 \\ 
		{\small BM25-adpt} & \textbf{.2571} & \textbf{.3135} & \textbf{.2112} & .4133 & .2480 & .3533\\ 
		{\small $\text{TF}_{l\circ\delta\circ p}\times\text{IDF}$} & .2516 & .3084 & .1932 & \textbf{.4340} & .2465 & .3647\\ 
		\bottomrule
	\end{tabular}
\end{table}
The observed differences in effectiveness are small and can be entirely attributed to variations in the scoring function; our methodology fixes all other parts of the indexing pipeline (e.g., tag cleanup, tokenization, and stopwords). Both an ANOVA and Tukey’s HSD show no significant differences between any variant on all test collections. These results confirm the findings of~\citet{trotman-bm25}: effectiveness differences are unlikely an effect of the choice of the BM25 variant. Across the IR literature, we find differences due to more mundane settings (such as the choice of stopwords) tend to be larger than the differences we observe here. Although we find no significant improvements over the original~\citep{bm25-robertson} formulation, using a variant of BM25 that avoids negative ranking scores might still be worthwhile.

You might have caught that the effectiveness scores of ATIRE and Lucene (accurate) are the same. This is not a mistake. As explained, the $k_1+1$ in ATIRE scales the scores linearly and does not affect the ranking. So the only difference that can change the effectiveness scores is the different $\mathit{idf}$ functions. However, these are practically the same, especially when a collection has a large number of documents ($N$):

\begin{align}
	\log\left(\frac{N}{\mathit{df}_t}\right) &= \log\left(\frac{N-\mathit{df}_t+\mathit{df}_t}{\mathit{df}_t}\right) \\
	&= \log\left(\frac{N-\mathit{df}_t}{\mathit{df}_t} + \frac{\mathit{df}_t}{\mathit{df}_t}\right) \\
	&= \log\left(\frac{N-\mathit{df}_t}{\mathit{df}_t} + 1\right) \\
	&\approx \log\left(\frac{N-\mathit{df}_t+0.5}{\mathit{df}_t+0.5} + 1\right)
\end{align}

Switching our attention from effectiveness to efficiency,
\Cref{bm25_effiency} presents the average retrieval time per query in milliseconds (without standard deviation for Anserini, which does not report time per query). MonetDB uses all cores for inter- and intra-query parallelism, while Anserini is single-threaded.

\begin{table}
	\centering
	\caption{Average retrieval time per query in ms: Anserini (top) and OldDog (bottom)}
	\label{bm25_effiency}
	\begin{tabular}{l | c c c}
		\toprule
		&Robust04&Core17&Core18\\
		\midrule
		Lucene (default)&52&111&120\\
		Lucene (accurate)&55&115&123\\
		\midrule
		\citeauthor{bm25-robertson}&$158\pm25$&$703\pm162$&$331\pm96$\\
		Lucene (default)&$157\pm24$&$699\pm154$&$326\pm90$\\
		Lucene (accurate)&$157\pm24$&$701\pm156$&$324\pm88$\\
		ATIRE&$157\pm24$&$698\pm159$&$331\pm94$\\
		BM25L&$158\pm25$&$697\pm160$&$333\pm96$\\
		BM25+&$158\pm25$&$700\pm160$&$334\pm96$\\
		BM25-adpt&$158\pm24$&$700\pm157$&$330\pm92$\\
		TF$_{l\circ\delta\circ p}\times$IDF&$158\pm24$&$698\pm158$&$331\pm96$ \\
		\bottomrule
	\end{tabular}
\end{table}

Comparing Lucene (default) and Lucene (accurate), we find negligible differences in effectiveness. However, the differences in retrieval time are also negligible, which calls into question the motivation behind the original length approximation. Currently, the similarity function and, thus, the document length encoding are defined at index time. Storing exact document lengths would allow for different ranking functions to be swapped at query time more effortlessly, as no information would be discarded at index time. Accurate document lengths might additionally benefit downstream modules that depend on Lucene. We suggest that Lucene might benefit from storing exact document lengths.

\section{Conclusion}
In summary, the previous sections describe a double reproducibility study. The study methodologically validated the usefulness of databases for IR prototyping and performed a large-scale study of BM25 to confirm the findings of \citet{trotman-bm25}. It does not seem to matter which of the multitude of BM25 variants is used.
Furthermore, to return to our research question, we can conclude that using relational databases for information retrieval is beneficial. Because data processing and storage are separated in relational databases, comparing different ranking functions in a relational system is much easier compared to a system that uses an inverted index. The work by \cite{OldDog} also confirmed that relational databases could be as efficient as inverted indexes in retrieval tasks. In short, databases have use cases in which they are easier to work with, while it remains possible to obtain efficient retrieval systems.

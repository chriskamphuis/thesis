\chapter{Creation of the Entity Graph}
\label{a-graph-of-entities}

\epigraph{``Is this new question a worthy one to investigate?'' This latter question we investigate without further ado, thereby cutting short an infinite regress.}{Alan Turing - 1950}


\begin{Abstract}
	\begin{changemargin}{1cm}{1cm}
		REBL is an extension of the Radboud Entity Linker (REL) for Batch Entity Linking. 
		REBL is developed after encountering unforeseen issues when trying to link the large MS MARCO v2 document collection with REL. In this chapter we discuss the issues we ran into and our solutions to mitigate them. REBL makes it easier to isolate the GPU heavy operations from the CPU heavy operations, by separating the mention detection stage from the candidate selection and entity disambiguation stages. By improving the entity disambiguation module we were able to lower the time needed for linking documents by an order of magnitude.
	\end{changemargin}
\end{Abstract}

\section{Introduction}
In the previous chapter we have introduced GeeseDB, a system that can query IR graphs and create rankings. We have used the system to express queries over the Washington Post news article collection. The examples we showed made use of entity annotations that were created by the Radboud Entity Linker (REL)~\citep{rel}.
Although, the Washington Post collection has been used for retrieval experiments, it is mostly used as a benchmark for news retrieval. 
In recent years, the MS MARCO ranking collections have become the de-facto benchmark for information retrieval experiments that employ deep learning. These collections are considerably larger than the Washington Post collection; the Washington Post collection consists of 728,626 news articles and blog posts, while the MS MARCO v2 document collections consists of almost 12 million web documents. The MS MARCO v2 document collections has almost 20 times as many documents, which are also longer on average.
When using REL, the Washington Post collection can be linked in a relatively short time. Using one GPU\footnote{NVIDIA GeForce RTX 3090} the linking time was amounted to approximately one day (24 hours). As the documents in the MS MARCO v2 document collection are longer than those in the Washington Post collection, we expected it would take about a month of time to link this collection using the same system setup. As it is possible to divide the workload over multiple machines, the time needed to link all documents can be decreased easily by deploying more servers. 

When starting to link the first segment of this collection however, the time it took to link was considerably larger than expected. In fact, after 72 hours there was a time-out after only linking 20 percent of the segment (200k documents), a processing job that should only take about 12 hours (following a rough estimation). Why entity linking took so much more time than estimated by extrapolation was unclear. 

The goal of the research described in this chapter concerns our approach to understand and mitigate these efficiency issues, such that we could link the MS MARCO v2 document collection in an acceptable runtime. 
In databases runtime efficiency is often improved through set-at-a-time operations instead of tuple-at-a-time operations. This means that instead of doing all computations for one data entry at a time, one computation is calculated for all data entries, before starting the following computation. 

We investigate whether the set-at-a-time execution model from database research might be applicable in the context of entity linking as well. Specifically, we developed a batch extension for REL dubbed REBL. REBL splits different stages in entity linking such that partial computations for the whole dataset can be calculated before starting the next computation.
REBL improves REL efficiency by almost an order of magnitude, decreasing the processing time per document (excluding mention detection) on a sample of 5000 MS MARCO documents from 1.23 seconds to 0.13 seconds. Given modest computational resources, we demonstrate that REBL enables the annotation of a large corpus like MS MARCO v2. We discuss potential improvements that can be made to further improve batch entity linking efficiency. The REBL code and toolkit are available publicly at \url{https://github.com/informagi/REBL}.

Our third research question; 
\begin{itemize}
	\item \emph{Research Question 3: When does information retrieval research benefit from graph data?} 
\end{itemize} 
is not directly answered by the research described in this chapter. The research in this chapter was needed in order to obtain a dataset that we want to approach the third question. So, it forms the basis for the research described in the next chapter that will try to answers this question. In order to give proper context for the research described in this chapter, first entity linking will be described in more depth, before explaining the Radboud Entity Linking system that was used for this research. 

\section{Related Work}\label{sec:related-work}
Entity linking concerns identifying entity mentions in text and linking them to their corresponding entities in a knowledge graph. It fulfills a key role in the knowledge-grounded understanding of documents. It has been proven effective for diverse tasks in information retrieval~\citep{Gerritse:2020:GEER, Gerritse:2022:EMBERT, doc-ranking-entity, el-ranking-hasibi, el-balog, query-recommendation-entity, chatterjee2022bert}, natural language processing~\citep{lin-etal-2012-entity, watson}, and recommendation~\citep{yang-etal-2018-collective}.
% Maybe I should write out all these examples, if time permits. 
Utilizing entity annotations in these downstream tasks depends upon the annotation of text corpora with a method for entity linking. Due to the complexity of entity linking systems, this process is often performed by reliance on a third-party entity linking toolkit, where examples include DBpedia Spotlight~\citep{dbpedia-spotlight}, TAGME~\citep{tagme}, Nordlys~\citep{nordlys}, GENRE~\citep{genre}, Blink~\citep{blink}, and REL~\citep{rel}.

A caveat in existing entity linking toolkits is that they are not designed for batch processing large numbers of documents. Existing entity linking toolkits are primarily optimized to annotate individual documents, one at a time. This severely restricts utilizing state-of-the-art entity linking tools, such as REL, Blink, and GENRE, that employ neural approaches and require GPUs for fast operation. Annotating millions of documents incurs significant computational overhead, to the extent that annotation of a large text corpus becomes practically infeasible using modest computational power resources, especially when tagging documents one by one. Batch entity linking is, however, necessary to build today's data-hungry machine learning models, considering large text corpora like the MS MARCO v2 (12M Web documents)~\citep{msmarco}, or the newer ClueWeb22 collection~\citep{clueweb22}.

\section{REL}
This research specifically concerns the Radboud Entity Linking (REL) toolkit, in the context of processing large corpora. REL is a state-of-the-art entity linking system as shown emperically and independently by \citet{bast-etal-2023-fair}. Its system architecture is similar to that of competing systems, including BLINK~\citep{blink}.
REL annotates individual documents efficiently, requiring only modest computational resources while performing competitively compared to the state-of-the-art methods on effectiveness. It considers entity linking as a modular problem consisting of three stages:

\begin{itemize}
	\item \emph{Mention Detection.} 
	This step aims to identify all possible text spans in a document that might refer to an entity. If text spans that refer to entities are not appropriately identified, the system cannot correctly link the entity in later stages. REL is built to be a modular system, making it possible to use different named entity recognition (NER) systems for this stage. For REL the default system used is FLAIR~\citep{flair}, which is a state-of-the-art NER system that uses contextual word embeddings. 
	\item \emph{Candidate Selection.}
	For every detected mention, REL considers up to $k_1 + k_2 (=7)$ candidate entities. $k_1 (=4)$ candidate entities are selected based on their a priori occurrence probability $p(e|m)$ (for entity \textit{e} given mention \textit{m}). These priors are pre-calculated by summing Wikipedia hyperlinks and priors from the CrossWiki corpus~\citep{crosswiki}. The other $k_2 (=3)$ entities are chosen based on the similarity of their entity embedding to the word embeddings of the context. In this stage, a context of a maximum of 200-word tokens is considered.
	\item \emph{Entity Disambiguation.}
	The final step aims to map the mention to the correct entity in a knowledge base. The candidate entities for each mention are obtained from the previous stage. REL implements the Ment-norm method proposed by~\citet{ED-paper}.  
\end{itemize} 

After all entities have been found, REL produces a JSON object that contains the entities and their respective locations in the source text using standoff annotation. Because of REL's modular architecture, it is an ideal system to deploy set-at-a-time. 

\section{From REL to REBL}
The MS MARCO v2 document collection contains 11,959,635 documents split into 60 compressed files, totaling roughly 33GB. Uncompressed, these files are in JSON line format (where every line represents a document, described by a JSON object). These JSON objects have five fields: \textit{url}, \textit{title}, \textit{headings}, \textit{body}, and \textit{docid}. We wanted to link the documents' titles, headings, and bodies for our experiments. We link to the 2019-07 Wikipedia dump, one of the two dumps also used in the initial development of REL. 

In order to ease linking this size of data, we separated the GPU-heavy mention detection stage from the CPU-heavy candidate selection and entity disambiguation stages; the modified code can be found on GitHub.\footnote{\url{https://github.com/informagi/REBL}, last accessed November 18th 2024}
For REBL, the input for mention detection are the compressed MS MARCO v2 document files, and its output consists of the mentions found and their location in the document, in Apache Parquet format.\footnote{\url{https://github.com/apache/parquet-format}, last accessed November 18th 2024}
These files and the source text are the input for the subsequent phases (candidate selection and entity disambiguation). The final output consists of Parquet files containing spans of text and their linked entities. 
In the following section, we discuss what is changed for mention detection, candidate selection, and entity disambiguation steps to make REL more suited to annotate large collections with entity links, in a batch processing manner.  

\subsection{Mention Detection}
REL~\citep{rel} uses Flair~\citep{flair} as the default method for mention detection, a state-of-the-art named entity recognition system.
In this section, we focus on inefficiencies that arise when interfacing between REL and flair. 
Flair uses the \texttt{segtok}\footnote{\url{https://github.com/fnl/segtok}, last accessed November 18th 2024} package to segment an (Indo-European) document in sentences, internally represented as \texttt{Sentence} objects. These sentences are split into words/symbols represented as \texttt{Token} objects.
When creating these representations, however, it is not possible to recreate the source text properly, as 
Flair adjusts the representations in its internal processing of text data. In order to have full control of these representations we decided to  construct the underlying data structures for REBL ourselves. To do this, we used the \texttt{syntok}\footnote{\url{https://github.com/fnl/syntok}, last accessed November 18th 2024} package, a follow-up version of \texttt{segtok}.
Both packages were developed by the same author, who claims that the \texttt{syntok} package segments sentences better than \texttt{segtok}. 
Using this new representation we fixed two problems: 
\begin{enumerate}
	\item Flair removes white space characters when multiple occur after each other, REL accounts for this by counting the number of white spaces characters removed, a somewhat inefficient process prone to introduce inaccuracies in the mapping between source text and the annotated result. Using our own representation we know the start of every token, and we do not need an additional system that tracks the number of removed white spaces. So we change the interface between Flair and REL, as we create the Flair objects that REL uses manually. 
	\item Various zero width Unicode characters are removed by Flair from the source text before creating a token: zero width space (\texttt{U+200B}), zero width non-joiner (\texttt{U+200C}), variation selector-16 (\texttt{U+FE0F0}), and zero width no-break space (\texttt{U+FEFF}). 
	These characters occur rarely, but in a collection as large and diverse as MS MARCO v2, these characters do occur. When encountering these characters using REBL, the token objects were constructed such that the span and offset of the token still referred to that of the source text:
	For the case of the zero width space, we updated the \texttt{syntok} package. While, according to the Unicode standard, zero width space is not a whitespace character, it should be considered a character that separates two words. For the other Unicode characters removed by Flair, we manually update the span in the \texttt{Token} objects created by Flair such that they refer correctly to the positions in the source text. Now, when Flair identifies a series of tokens as a possible mention, we can directly identify the location in the source text from the \texttt{Token} objects.
\end{enumerate}

In order to efficiently use GPU resources, it is important to create batches of data to decrease the number of I/O operations. Every time a GPU calculation is called, data needs to be transferred from and to the GPU hardware. 
Flair does in fact support named entity recognition in batches; multiple pieces of text can be sent to the GPU, to achieve an overall faster inference time (as fewer I/O operations are needed). 
Because REL was designed to tag one document at a time, it did not utilize this functionality. REBL exploits this feature, allowing users to specify the number of documents to be tagged simultaneously, increasing linking efficiency. 

\subsection{Candidate Selection and Entity Disambiguation}
For candidate selection, REL makes use of a $p(e|m)$ prior, where \textit{e} is an entity, and \textit{m} is a mention. These priors are saved in an (SQLite) database, and up to 100 priors per mention are considered. However, data conversion between the client and the representation stored in the database incurred a high serialization cost. We updated this to a format that is faster to load, with the additional benefit of a considerably decreased database size.\footnote{The table that represents the priors shrank from $9.6$GB to $2.2$GB.}
We experimented with data storage in the DuckDB column-oriented database as an alternative. However, we found that SQLite was (still) more efficient as a key-value store, at least in DuckDB's state of development when we ran the experiments.

The entity disambiguation stage took much longer than reported in the original REL paper. This difference can partially be explained by the length of the documents to be linked. The documents evaluated by~\citet{rel} consisted of, on average, 323 tokens, with an average of 42 mentions to consider. The average number of tokens in an MS MARCO v2 document is about $1,800$, with 84 possible mentions per document.\footnote{These figures are calculated over the body field; we also tagged the shorter title and headers fields.}
Per mention, 100 tokens to the left and the right (so 200 total) are considered as context for the disambiguation model. 
The longer documents result in a higher memory consumption per context and document, with higher processing costs as the result.

We improve the efficiency of the entity disambiguation step such that it can be run in a manageable time. REL recreated a database cursor every time candidates where being generated. We rewrote the REL database code to create a single database cursor for the entity disambiguation module. 
When a mention occurs multiple times within a document, the exact same queries were issued to the database multiple times within a document.  By caching the output of these queries, we could considerably lower the number of database calls needed. We ended up caching all database calls for a segment, as we ran the process for every segment separately. 

The default setting of REL is to keep embeddings on the GPU after they are loaded, also ones that are not needed for disambiguation for following batches.
This design decision, however, slowed down disambiguation when many documents were being processed consecutively, because operations like normalization were carried out over all embeddings on the GPU. 
A considerable speed-up has been achieved by clearing these embeddings as soon as a document is processed.

Finally, after retrieving the embeddings from the database, REL puts them in a Python list. We rewrote the REL code such that the binary data is directly loaded from NumPy, a data format that Pytorch can use directly. 

\section{Effects on Execution}
In the mention detection stage, we improved tokenization and applied batching. The MS MARCO v1 collection does not contain characters that cause the problem in tokenization; the documents in that version of the collection were sanitized before it was published. In the MS MARCO v2 collection, $411,906$ documents have tokens that would be removed automatically by Flair, which corresponds to $3.4\%$ of all documents.
Batching documents in the mention detection stage decreased the average time for finding all named entities. We used batches of size 10, as the documents are relatively large. The optimal batch size will depend on the available GPU memory, and the length of the documents that need to be processed.

Some documents in the MS MARCO v2 collection cannot be linked. This happens only in extraordinary cases where linking with entities did not make sense in the first place, an example being a document consisting of numbers only. Here, the \texttt{syntok} package created one long \texttt{Sentence} object from this file that could not fit in GPU memory.

Table \ref{tab:efficiency} shows our improvements to the candidate selection and entity disambiguation step and describes how much time is saved in REBL. 
The code improvements to create the database cursor only once and to load the data directly from NumPy had no noticeable effect on the overall run time of entity disambiguation and are not reported in this table. This is likely because the costs of these operations are quite low, relatively to the efficiency improvements made by the other changes.
Note that the large standard deviations are primarily due to the differences in processing costs between long and short documents.

\begin{sidewaystable}
	\caption{Efficiency improvements for Candidate Selection and Entity Disambiguation. Improvements are calculated over a sample of 5000 documents using a machine with an Intel Xeon Silver 4214 CPU @ 2.20GHz using two cores with 187GB RAM and a GeForce RTX 2080 Ti (11GB) GPU. Improvements are cumulative; the times shown include the previous improvement as well.}
	\label{tab:efficiency}
	\begin{tabular}{p{6cm} c p{10cm}}
		\toprule
		Improvement & Seconds & Explanation\\
		\midrule
		Old Candidate Selection + Entity Disambiguation & $1.23 \pm 2.09$ & Average time it takes to select candidates and disambiguate per document\\
		\midrule
		No embedding reset & $0.26 \pm 1.60$ & The default setting of REL was to keep embeddings in GPU memory after they were loaded by clearing them from GPU memory after every document a speed up was achieved.\\
		Cache database calls & $0.15 \pm 1.31$ & When an entity occurs within a document, there is a high probability of it occurring multiple times. By caching the calls, we increase memory usage but can lower the time needed for candidate selection and entity disambiguation.  \\
		Representation change candidates & $0.13 \pm 1.19$ & By representing the candidates better in the database, we were able to save on conversion time, lowering the time needed for candidate selection.\\
		\bottomrule
	\end{tabular}
\end{sidewaystable}

\section{Conclusion and Discussion}
This chapter described REBL, an extension for the Radboud Entity Linker. We utilized REL's modular design to separate the GPU-heavy mention detection stage from the CPU-heavy candidate selection and entity disambiguation stages. The mention detection module is now more robust and reliable, using a better segmenter and preserving location metadata correctly.
The candidate selection and entity disambiguation steps were updated to improve their runtime Although it is now possible to run REL.~\citep{rel} on MS MARCO v2~\citep{msmarco} in a (for us) somewhat reasonable time, we identified further improvements to implement.

In the candidate selection step, found mentions are compared to all other mentions. The complexity of this step is $O(n^2)$, with $n$ being the number of mentions found in a document, which is especially problematic for longer documents. As we are only interested in similar mentions, it may be worthwhile to implement a locality-sensitive hashing algorithm to decrease the number of comparisons needed at this stage. 

Per mention, all context tokens are considered. This context has to be constructed from the source document. As a result, we load the source data a second time during candidate selection. Alternatively, we could output the mention context in the mention detection stage, which could speed up the candidate selection stage as we do not have to reconstruct the context for a second time. However, this would significantly increase the size of the mention detection output. More experiments are needed to strike the right balance here. 
Having a streaming approach would probably mitigate this issue. Another way a streaming approach might benefit REBL is that now, because a two-step approach is being used, intermediate results are written to the file system in parquet format. 

Overall, it has become clear that a data processing-oriented perspective on entity linking is necessary for efficient solutions. A set-at-a-time approach should be preferred over a tuple-at-a-time approach when large amounts of data have to be processed. 